在序列学习中，我们以往假设的目标是： 在给定观测的情况下 （例如，在时间序列的上下文中或在语言模型的上下文中）， 对下一个输出进行建模。 虽然这是一个典型情景，但不是唯一的。
例如上一个句子开头是Green，但我们需要通过*下文*才能够直到Green代表的是一个人还是绿色。

# 架构和定义
在系统设计上，我们需要下一个时间步的隐状态：
![[assets/Pasted image 20230925145257.png|500]]
对于给定的时间步$t$，假设有一组输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$（其中，$n$表示样本数，$d$表示输入数）。另设该时间步的前向和反向隐状态分别为$\overrightarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$和$\overleftarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$（其中，$h$是隐藏单元数）。则有：
$$
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}
$$
将前向和反向隐状态合并，得到输出到下一层的隐状态$\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$：
$$
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.
$$
> 需要注意这里的张量大小。

# 代价和限制
双向循环神经网络的一个关键特性是：使用来自**序列两端的信息来估计输出**。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。 

但是在对下一个词元进行预测的情况中，这样的模型*并不是我们所需*的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。

另一个严重问题是，双向循环神经网络的**计算速度非常慢**。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。

# 应用
双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如：
- 填充缺失的单词、词元注释。
- 


# 隐马尔可夫模型的动态规划
假设在任意时间步$t$，假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$来控制我们观察到的$x_t$。此外，任何$h_t \to h_{t+1}$转移都是有一些状态转移概率$P(h_{t+1} \mid h_t)$给出的，则称为**隐马尔可夫模型**，如图所示：
![](Pasted%20image%2020230924122059.png)
因此，对于有$T$个观测值的序列（一般指长度为$T$的序列），我们在观测状态和隐状态上具有以下联合概率分布可以通过公式给出：
$$
P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1)
$$

假设通过$x_{-j}$表示非时间步$j$的观测所组成的向量，即：
$$
x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})
$$
显然，目标其实是计算$P(x_j \mid x_{-j})$。由于没有观测到时间步$j$，$P(x_j \mid x_{-j})$不含隐变量，因此考虑对$h_1, \ldots, h_T$选择构成的*所有可能*的组合进行求和。而如果任何$h_i$可以接受$k$个（有限）不同的值，意味着需要对$k^T$个项求和，这里就需要引入**动态规划**。
我们将求和公式逐项展开，得到：
$$
\begin{split}\begin{aligned}
    &P(x_1, \ldots, x_T) \\
    =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\
    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\
    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \mid h_1) P(h_2 \mid h_1)\right]}_{\pi_2(h_2) \stackrel{\mathrm{def}}{=}}
    P(x_2 \mid h_2) \prod_{t=3}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\
    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \mid h_2) P(h_3 \mid h_2)\right]}_{\pi_3(h_3)\stackrel{\mathrm{def}}{=}}
    P(x_3 \mid h_3) \prod_{t=4}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t)\\
    =& \dots \\
    =& \sum_{h_T} \pi_T(h_T) P(x_T \mid h_T).
\end{aligned}\end{split}
$$
其中，前向回归写为：
$$
\pi_{t+1}(h_{t+1}) = \sum_{h_t} \pi_t(h_t) P(x_t \mid h_t) P(h_{t+1} \mid h_t).
$$
初始化第一项为$\pi_1(h_1) = P(h_1)$。

