在torch的Optimizer中，特别是SGD中，可以指定`weight_decay`参数，即[[权重衰减]]公式中的$\lambda$参数。
```python
def __init__(self, params, weight_decay=0, ...):
```

另外，通过将`weight_decay`加入到params中，可以为指定的值设置衰减：
```python
trainer = torch.optim.SGD([
	{"params":net[0].weight,'weight_decay': wd},
	{"params":net[0].bias}], lr=lr)
```
> 只为权重设置衰减