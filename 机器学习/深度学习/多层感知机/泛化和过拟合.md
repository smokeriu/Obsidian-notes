# 过拟合
如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在*训练集上达到完美的精度*，此时*测试集的准确性却下降了*，这种情况，即模型在训练数据上拟合的比在潜在分布中更接近的现象称为*过拟合*，用于对抗过拟合的技术称为*正则化*。

# 训练误差和泛化误差
*训练误差*（training error）是指， 模型在训练数据集上计算得到的误差。
*泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由**随机选取**的、**未曾在训练集中出现**的数据样本构成。

# 模型复杂性
通常对于神经网络，我们认为需要*更多训练迭代*（epoch）的模型比较复杂， 而需要*早停*（early stopping）的模型（即较少训练迭代周期）就不那么复杂。
我们很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。 就目前而言，一条简单的经验法则相当有用： 统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。

根据经验，下述问题可能导致过拟合
1. 可调整参数的数量（过多）。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
2. 参数采用的值（范围过大）。当*权重*的*取值范围*较大时，模型可能更容易过拟合。
3. 训练样本的数量（过少）。即使模型很简单，也很容易过拟合*只包含一两个样本的数据集*。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

# 过拟合和欠拟合
## 欠拟合
1. 训练误差和验证误差都很严重， 但它们之间仅有一点差距，且如果模型*不能降低训练误差*，这可能意味着模型过于简单（即表达能力不足）。
2. 训练和验证误差之间的*泛化误差*很小。
这意味着，可以用一个更复杂的模型降低训练误差。 这种现象被称为*欠拟合*（underfitting）。

## 过拟合
