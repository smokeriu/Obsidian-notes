在数学中，两个函数（比如$f, g: \mathbb{R}^d \to \mathbb{R}$）之间的**卷积**被定义为：
$$
(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.
$$
也就是说，卷积是当把一个函数“翻转”并移位$\mathbf{x}$时，测量$f$和$g$之间的**重叠**。当为**离散对象**时，积分就变成**求和**。

例如，对于由索引为$\mathbb{Z}$的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：
$$
(f * g)(i) = \sum_a f(a) g(i-a).
$$
对于二维张量，则为$f$的索引$(a,b)$和$g$的索引$(i-a, j-b)$上的对应加和：
$$
(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).
$$
这与卷积层的定义很接近，只不过符号不一致。不过，相同的是，卷积所体现的，更多是*互相关*。

卷积的理解，可以参考：[从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变](https://www.bilibili.com/video/BV1VV411478E/?from=search&seid=1725700777641154181&vd_source=8fc1ba7db0ecc643e9aacb066fe696cb)。

# 总结
卷积运算，在深度学习中，我们可以理解为：**计算一个元素预期周围元素的相关性**。[[互相关运算]]是这种思想的具体表现。