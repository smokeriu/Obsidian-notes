逻辑回归，指使用数学来找出**两个**数据因子之间的关系，一般用于解决*二分类*的问题。

# 模型函数
逻辑回归一般是由线性回归和[激活函数](激活函数.md)组成。公式为：
$$
\hat{y} = \sigma(w^T x + b)
$$
其中：
- $\sigma$为激活函数。
- $w$为权重参数。是一个学习参数。
- $b$为偏置。是一个学习参数。
- $\hat{y}$表示模型输出的结果，且有：$\hat{y} \in (0,1)$。


# 损失函数
在二分类问题中，显然我们要预测的结果是0或1，本质上可以认为这是一个概率问题：已知观测数据，得到这个结果的*概率*是多少。
则对于一条输入，结果为1的概率：
$$
P(Target = 1 \mid x_i) = \hat{y_i}
$$
而对于一条输入，结果为0的概率：
$$
P(Target = 0 \mid x_i) = 1 - \hat{y_i}
$$
其中：
- $\hat{y_i}$表示对于第$i$个样本，模型输出的值。

显然，对于深度学习，是在已知样本结果的基础上，来推测模型应该长什么样，则可以翻译为[似然函数](似然函数.md)，并求极大似然估计，由于二分类问题符合[伯努利分布](伯努利分布.md)，则其概率符合如下分布：
$$
P(w_i) = \hat{y_i}^{y_i} (1 - \hat{y_i})^{1-y_i}
$$
则似然函数为：
$$
L(w) = \prod \hat{y_i}^{y_i} (1 - \hat{y_i})^{1-y_i}
$$
为了简化求导计算，一般会对上述公式取对数，转换为加法，得到：
$$
L(w) = \sum[y_i ln{\hat{y_i}} + (1-y_i)ln{(1-\hat{y_i})}]
$$
显然，我们的目标是找到上述公式中，最符合条件的