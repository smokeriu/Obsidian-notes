考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。 每一层$l$由变换$f_l$定义， 该变换的参数为权重$\mathbf{W}^{(l)}$， 其隐藏变量是$\mathbf{h}^{(l)}$（特别的，令$\mathbf{h}^{(0)} = \mathbf{x}$）。
> 所谓隐藏变量，其实指的是隐藏层的输出。

则该网络表示为：
$$
\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) , \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).
$$
> 没有找到关于符号$\circ$的准确定义，但不影响这个公式的理解。

如果隐藏变量和输入都是向量，则输出对于任意权重矩阵的梯度，有：
$$
\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.
$$
换言之，该梯度是$L-l$个矩阵$\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$与梯度向量$\mathbf{v}^{(l)}$的乘积。

> 这里本质是根据[[../../../数学/基础/微积分/链式法则|链式法则]]求导。不过我们将最后一项看成$\mathbf{v}^{(l)}$。



# 参考
- [详解深度学习中的梯度消失、爆炸原因及其解决方法 - 知乎](https://zhuanlan.zhihu.com/p/33006526)