优化器主要有两类：随机梯度下降（SGD）和自适应学习率（Ada）。
# 随机梯度下降
随机梯度下降到理念是，当训练出现震荡时，此时减小学习率，使得模型可以快速收敛。其公式为：
$$


# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。


# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。