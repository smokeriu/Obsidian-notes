优化器主要有两类：随机梯度下降（SGD）和自适应学习率（Ada）。
# 随机梯度下降
随机梯度下降到理念是，当训练出现震荡时，此时减小学习率，使得模型可以快速收敛。其公式为：
$$
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)
$$
$$

$$
其中：

解释：
当梯度$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$为正值，说明处于梯度下降的过程中，则会使得

# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。


# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。