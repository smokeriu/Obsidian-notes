反向传播，指的是计算神经网络参数[[../../../数学/基础/微积分/梯度公式|梯度]]的方法，该方法根据微积分中的*链式规则*，按相反的顺序从输出层到输入层遍历网络。

假设我们有函数$\mathsf{Y}=f(\mathsf{X})$和$\mathsf{Z}=g(\mathsf{Y})$，利用链式法则，我们可以计算$\mathsf{Z}$关于$\mathsf{X}$的导数：
$$
\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)
$$
> prod是一个抽象的运算符，表示在执行必要的操作（如换位和交换输入位置）后将其参数相乘。

数据经过[[前向传播]]，得到了目标函数$J=L+s$。我们需要利用反向传播，计算各个值的梯度。

第一步，我们计算其对损失函数$\mathbf{L}$和正则化$\mathbf{s}$的梯度：
$$
\frac{\partial J}{\partial L} = 1 \; \text{and} \; \frac{\partial J}{\partial s} = 1.
$$
第二部，根据[[../../../数学/基础/微积分/链式法则|链式法则]]，计算输出层的梯度：
$$
\frac{\partial J}{\partial \mathbf{o}}
= \text{prod}\left(\frac{\partial J}{\partial L}, \frac{\partial L}{\partial \mathbf{o}}\right)
= \frac{\partial L}{\partial \mathbf{o}}
\in \mathbb{R}^q.
$$
> 因为输出$\mathbf{o}$是长度为$q$的向量，损失向量是一个关于输出的向量，根据偏导原理，二者的导数的结果也是长度为$q$的向量（不相关项在偏导时可以认为是常数项，则导数为0）。

第三步，计算*正则化*项相对于两个参数的梯度：
$$
\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}
\; \text{and} \;
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.
$$
> 这里使用的是$L_2$正则，$s=\frac{\lambda}{2} \|\mathbf{w}\|^2$，根据[[../../../数学/基础/微积分/梯度公式#对于$L_2$范数的梯度|梯度公式]]，有$\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = 2\mathbf{x}$。

将第二步和第三步结合，计算输出层的权重梯度$\partial J/\partial \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}$，得到：
$$
\frac{\partial J}{\partial \mathbf{W}^{(2)}}= \text{prod}\left(\frac{\partial J}{\partial \mathbf{o}}, \frac{\partial \mathbf{o}}{\partial \mathbf{W}^{(2)}}\right) + \text{prod}\left(\frac{\partial J}{\partial s}, \frac{\partial s}{\partial \mathbf{W}^{(2)}}\right)= \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.
$$
> 对于第一项中，$\mathbf{o}= \mathbf{W}^{(2)} \mathbf{h}$，根据[[../../../数学/基础/微积分/梯度公式#规则1|梯度公式]]，有如上结果。

