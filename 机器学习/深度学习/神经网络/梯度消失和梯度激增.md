# 梯度消失
随着我们增加神经网络的隐藏层数量，我们会发现并没有对学习产生正面影响，这一般是梯度消失引起的。
## 原因
其实主要是用[[感知器和神经元#S型神经元]]引起的，对$\sigma$函数求导数，会得到如下图像：
![[assets/Pasted image 20230802162319.png|500]]
特别的，我们对代价函数求某一神经元的偏置的导数，我们会发现导数与$w_j \sigma$相关


# 梯度激增
与梯度消失相对应的就是梯度激增，


# 解决