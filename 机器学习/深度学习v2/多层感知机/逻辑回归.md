逻辑回归，指使用数学来找出**两个**数据因子之间的关系，一般用于解决*二分类*的问题。

# 模型函数
逻辑回归一般是由线性回归和[激活函数](../../深度学习/多层感知机/激活函数.md)组成。公式为：
$$
\hat{y} = \sigma(w^T x + b)
$$
其中：
- $\sigma$为激活函数。
- $w$为权重参数。是一个学习参数。
- $b$为偏置。是一个学习参数。
- $\hat{y}$表示模型输出的结果，且有：$\hat{y} \in (0,1)$。


# 损失函数
在二分类问题中，显然我们要预测的结果是0或1，本质上可以认为这是一个概率问题：已知观测数据，得到这个结果的*概率*是多少。
则对于一条输入，结果为1的概率：
$$
P(Target = 1 \mid x_i) = \hat{y_i}
$$
而对于一条输入，结果为0的概率：
$$
P(Target = 0 \mid x_i) = 1 - \hat{y_i}
$$
其中：
- $\hat{y_i}$表示对于第$i$个样本，模型输出的值。

显然，对于深度学习，是在已知样本结果的基础上，来推测模型应该长什么样，则可以翻译为[似然函数](似然函数.md)，并求[极大似然估计](极大似然估计.md)，由于二分类问题符合[伯努利分布](伯努利分布.md)，则其概率符合如下分布：
$$
F(w_i) = \hat{y_i}^{y_i} (1 - \hat{y_i})^{1-y_i}
$$
则似然函数为：
$$
L(w) = \prod \hat{y_i}^{y_i} (1 - \hat{y_i})^{1-y_i}
$$
为了简化求导计算，考虑到$\log$是单调的，一般会对上述公式取对数，转换为加法，得到：
$$
L(w) = \sum[y_i ln{\hat{y_i}} + (1-y_i)ln{(1-\hat{y_i})}]
$$
显然，我们希望有一个$w$，使$L(w)$越大越好。在优化中，我们更习惯求解最小值，因此只需要最小化上述公式，并取平均：
$$
J(w) = -\frac{1}{m}L(w)
$$
这个就是逻辑回归的损失函数。

> 逻辑回归因为简单，可以通过数学推导出最优解公式，所以损失函数主要是数学推导，和"损失"二字的关系其实不大。个人觉得目标函数更加恰当。

> 这里简化没有将偏置看做是函数参数。
## 计算
显然，为了得到上述函数的最小值，我们需要对其进行求导：
$$
g_i = \frac{\partial{J(w)}}{\partial{w_i}}
$$
在深度学习中，称$g_i$为**梯度**，通过求导计算损失函数最小值的过程称为**梯度下降**，而由于我们是通过损失函数反向求得*最佳*参数，所以这整个计算过程称为反向传播。
一般而言，还会引入一个**学习率**$\eta$，来加快或减缓下降的程度：
$$
\Delta w = \eta g_i
$$

# 代码
## 初始化参数
```python
def __init__(self, n_feature):  
    # 因为w需要与x相乘，其需要与x相同的feature个数。  
    self.w = torch.rand(size=[n_feature, 1], requires_grad=True)  
    # 偏置通过广播来自动匹配大小即可。  
    self.b = torch.rand(size=[1, 1], requires_grad=True)
```
- 因为w需要与x相乘，其需要与x相同的feature个数。 
- 偏置通过Torch的广播机制来自动匹配大小即可。  

## 前向传播
```python
def forward(self, x):  
    # 使用广播矩阵乘法，因为w的大小只有1。  
    # 转置是必须的，因为x的shape是(batch size, feature)  
    calc = torch.matmul(self.w.transpose(0, 1), x) + self.b  
    return F.sigmoid(calc)
```
- x的第二个维度才是feature，所以w需要转置。
- 这里需要使用[[../../PyTorch/基础/矩阵乘法#matmul|矩阵乘法]]，且两个参数的形状不一样，需要广播。

## 损失函数
```python
@staticmethod  
def loss_func(y_hat, y) -> torch.Tensor:  
    batch_size = y_hat.size(0)  
    return -(y * torch.log(y_hat) + (1 - y) * torch.log(1 - y_hat))/batch_size
```
- 这里是按位置乘，而不是数学上的矩阵乘法。

## 训练
```python
def train(self, epochs, eta, data, label):  
    for epoch in range(epochs):  
        for step in range(data.size(0)):  
            y_hat = self.forward(data[step])  
            loss = self.loss_func(y_hat, label[step])  
            loss.backward()  
            with torch.no_grad():  
                self.w.data -= eta * self.w.grad.data  
                self.b.data -= eta * self.b.grad.data  
            self.w.grad.data.zero_()  
            self.b.grad.data.zero_()  
        print("Epoch: %03d, Loss %.3f" % (epoch, loss.item()))
```

- 更新参数需要放置于不含偏移量的位置，避免更新参数的过程被纳入反响传播的过程。
	- 因为修改参数值，如果纳入梯度计算，则反向传播显然会进入递归，这显然是不对的。
- 更新完后，需要将梯度归零。
# 参考
- [损失函数的交叉熵与极大似然估计推导 - 知乎](https://zhuanlan.zhihu.com/p/458745814)