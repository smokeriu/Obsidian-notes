> Softmax回归其实是一个分类问题。

# Softmax定义
Softmax是一个函数，其作用于输出上。对于分类问题，网络会产生n个输出（对应n个类别），组成一个n维向量，向量中的每个值表示预测结果**属于这个分类的置信度**。

Softmax作用于这个输出向量$\mathbf{o}$上，得到一个预测向量$\hat{\mathbf{y}}$：
$$
\hat{\mathbf{y}} = softmax(\mathbf{o})
$$
而具体的Softmax函数，则作用于向量的每个元素上，可以写作：
$$
\hat{y_i} = \frac{e^{o_i}}{\sum_k e^{o_k}}
$$
其中：
- $k$表示最后会产生$k$个类别。
- $i$表示输出向量的第$i$维。
相当于将预测结果作为自然指数，这么做的目的：
- 保证每一项均$>0$。
- 保证$\hat{\mathbf{y}}$中各项总和为1。

# Softmax与交叉熵
Softmax一般会和[交叉熵代价函数](交叉熵代价函数.md)一起使用，即我们可以得到交叉熵**损失**函数：
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = 
- \sum_i y_i log{\hat{y_i}}
$$
其中：$\mathbf{y}$是目标值，其是一个n维向量，且*只有第$i$位为1，其余位为0*。表示结果为第$i$位表示的分类项。
所以，我们可以将上述公式简化为：
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = 
- log(\hat{y_y})
$$
对这个损失函数求梯度，得到：
$$
\partial_{o_i}l(\mathbf{y}, \hat{\mathbf{y}})
=
softmax(\mathbf{o})_i - y_i
$$
> 这里求导需要将Softmax函数带入损失函数公式，最后的结果可以替换回softmax函数。
> 另外，由于这里时对$o_i$求偏导，最好使用简化前的公式。如果使用简化后的，则需要区分$y_y$这项的定义，不能将其当做常数处理，而是分$y=i$和$y \ne i$两种情况。
> 证明暂时忽略。

所以，损失函数的梯度，可以看成是真实概率和预测概率的区别。
