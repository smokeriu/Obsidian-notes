> 又名Dropout。

在计算后续层之前向网络的*每一层注入噪声*。就是暂退法。

# 由来
## 重新审视过拟合
当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。不幸的是，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。

泛化性和灵活性之间的这种基本权衡被描述为*偏差-方差权衡*。线性模型有很高的偏差，但方差很低。
- 偏差很高：只能表示一小类函数。
- 方差很低：在不同的随机数据样本上可以得出相似的结果。

而我们的目标就是提高泛化性。
## 扰动的稳健性
经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以*简单的模型*为目标。
- 以较小维度的形式展现。
- 平滑性，即函数不应该对其输入的微小变化敏感。即*添加一些噪声*应当不会影响结果。
	- 已经从数学证明具有输入噪声的训练等价于Tikhonov正则化。

为此，在计算后续层之前向网络的每一层注入噪声，当训练一个有多层的深层网络时，注入噪声只会在`输入-输出`映射上*增强平滑性*。

因为我们从表面上看是在训练过程中丢弃一些神经元，所以被称为暂退法。标准暂退法包括在计算下一层之前*将当前层中的一些节点置零*。
# 定义
暂退法的关键在于注入噪声，采用无偏向的方式注入噪声，则每一层的期望值等于没有噪音时的值。
每个中间活性值$h$以暂退概率$p$由随机变量$h^{'}$替换，如下所示：
$$
\begin{split}\begin{aligned}
h' =
\begin{cases}
    0 & \text{ 概率为 } p \\
    \frac{h}{1-p} & \text{ 其他情况}
\end{cases}
\end{aligned}\end{split}
$$
其中，期望保持不变：$E[h'] = h$。

> 注意，这里的h即为该层的输出，我们需要保证输出的期望一致，所以相当于每一个输出都要进行”放大“。


# 应用
实际应用中，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。例如删除下图中的$h_2$和$h_5$节点，因此输出的计算不再依赖于这两个节点。
![[assets/Pasted image 20230808133246.png]]
它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于$h_1,…,h_5$的任何一个元素。