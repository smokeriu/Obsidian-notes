在线性神经网络中，我们处理图像是直接平铺开，这对于小图片还行，然而对于稍大一点的照片，例如百万级像素的照片，如果按照线性神经的办法，则会拥有一百万个维度。

卷积的概念，来源于不变性：
- 平移不变性：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。
	- 即**不管它的输入是如何平移的，系统产生完全相同的响应（输出）。**。
- 局部性：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系。


# 多层感知机的限制
首先，[多层感知机](多层感知机.md)的输入是二维图像$\mathbf{X}$，其*隐藏表示*$\mathbf{H}$在数学上是一个矩阵，在代码中表示为二维张量。为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。

> 隐藏表示，即输入经过处理后的输出。

使用$[\mathbf{X}]_{i, j}$和$[\mathbf{H}]_{i, j}$分别表示输入图像和隐藏表示中位置$(i,j)$处的像素。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵替换成*四阶*权重张量$\mathsf{W}$，并使用$\mathbf{U}$表示偏置参数，则可以得到公式：
$$
\begin{split}\begin{aligned} \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}\end{split}
$$
其中，第二个公式使用$a$和$b$替换了$k$和$l$，他们的关系是：
- $k = i+a$。
- $l = j+b$。
这么做的目的是，我们将输入和输出通过$(i,j)$坐标联系了起来，对于隐藏表示中任意给定位置$(i,j)$处的像素值$[\mathbf{H}]_{i, j}$，可以通过在$\mathbf{X}$中以$(i,j)$为中心对像素进行加权求和得到，而加权权重正是$[\mathsf{V}]_{i, j, a, b}$。

> 这里之所以是以$(i,j)$为中心，是因为我们选择的具体的$[\mathbf{X}]_{i+a, j+b}$是由$(i,j)$变换得来的，而由于我们定义的$(k,j)$求和包含了$\mathbf{X}$上的每一个像素。

> 进一步而言，对于不同的$(i,j)$取值，$(a,b)$取值会不一样。

## 平移不变形
这意味着检测对象在输入$\mathbf{X}$中的平移，应该仅导致隐藏表示$\mathbf{H}$中的平移。也就是说，**$\mathbf{U}$和$\mathbf{V}$实际上不依赖于$(i,j)$的值**。
即$[\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}$。并且偏置是一个常数，则上述公式简化为：
$$
[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b} [\mathbf{X}]_{i+a, j+b}.
$$

## 局部性
