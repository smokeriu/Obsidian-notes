# 文本序列

文本序列与[序列模型](序列模型.md)类似，例如，想要生成`deep learning is fun`这么一串文字，则可以抽象成概率计算：
$$
P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is})
$$
即，我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 这些概率本质上就是语言模型的参数。

对于常见的单词，想要得到概率，我们可以计算其在文本中出现的概率：
$$
\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})}
$$
其中：
- $n(x)$表示$x$出现的次数。
- $n(x, x^{'})$表示$x,x^{'}$单词对出现的次数。

显然，这种办法的问题在于：
- 对于一些不常见的单词组合，或更长的单词对组合，在数据集中却找不到。这会得到概率为0的结果。一种常见的策略是使用*拉普拉斯平滑*。
- 完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。
# 拉普拉斯平滑

一种常见的策略是使用*拉普拉斯平滑*。即在分母和分子上同时添加一个小常量，是结果一定不为0：
$$
\begin{split}\begin{aligned}
    \hat{P}(x) & = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\
    \hat{P}(x' \mid x) & = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}, \\
    \hat{P}(x'' \mid x,x') & = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}.
\end{aligned}\end{split}
$$
其中：
- $n$表示训练集中的单词总数。
- $m$表示单词去重和的总数。
- $\epsilon_1,\epsilon_2, \epsilon_3$是超参数。当其为0时，即不应用平滑。当其接近$+\infty$时，$\hat{P}(x)$接近均匀分布的$\frac{1}{m}$。
这解决了未出现的单词组合概率为0的问题。


# N元语法
