# 思想背景
随着函数的复杂，可能不会使网络更接近理想的结果：
![](Pasted%20image%2020230827173318.png)
对于左侧的情况，随着函数的复杂，其能够学习的范围扩大，但并没有离最佳结果（五角星）更近，所以，我们实际需要的泛函是第二类，即随着函数的复杂，其至少需要包含原来函数的表达范围，这样，更复杂的函数至少不会比原来函数结果更差。

ResNet正是这种思想的具体实现，其主要思想是：

我们假设函数输入为$x$，则有$f(x)$，我们在其上增加一级，得到：
$$
g(x) = x + f(x)
$$
这样，则$g(x)$一定包含了输入的$x$。即至少不会比原来差：

> 如果$f(x)$会导致结果变坏，则随着训练的进行，其得到的权重会逐渐减小（训练是朝着变好的方向进行），则最终，$g(x) \approx x$。

![](Pasted%20image%2020230827175533.png)

实际训练过程中，因为$g(x)$中，$x$比$f(x)$更简单，所以其更容易被训练，这也意味着模型会更快基于$x$得到一个*近似的模型*，再利用更深的$f(x)$进行*微调*。

# ResNet块
尽管ResNet的思想是**通用的**，不过其最初的版本是沿用了[使用块的网络(VGG)](使用块的网络(VGG).md)实现的，在具体的实现上，一般有两种选择——是否增加[1x1卷积层](1x1卷积层.md)：
![](Pasted%20image%2020230827182128.png)

# 问题
ResNet通过加法传递函数，这解决了网络过深时，容易[[../../多层感知机/不稳定的梯度#梯度消失|梯度消失]]的问题。但这会导致下层的参数（权重和偏置），会直接的传递到下一级，显性的，这会带来参数过多的问题。