如果一个神经元的净输入分布在神经网络中是动态变化的，比如循环神经网络，那么无法应用[[批量归一化层]]操作。另外批量归一化的效果还取决于小批量值$K$的选择。
层归一化和批归一化不同的是，层归一化是对一个中间层的**所有神经元**进行归一化。这意味着其效果可以不依赖于批量值$K$的选择。

对于第$l$隐藏层，其均值和方差为：
$$
\begin{aligned}&\mu^{(l)}=\frac1{n^{(l)}}\sum_{i=1}^{n^{(l)}}z_i^{(l)}\\\\&\sigma^{(l)^2}=\frac1{n^{(l)}}\sum_{k=1}^{n^{(l)}}(z_i^{(l)}-\mu^{(l)})^2\end{aligned}
$$

其中，$n^{(l)}$为第$l$层的神经元个数。而均值$\mu^{(l)}$与批量归一化的区别在于，批量归一化是在批次角度进行均值计算，我们假设输入的矩阵为$(B,N,C)$，则批归一化得到的均值矩阵结构为$(N,C)$，而层归一化得到的均值矩阵为$(B,C)$。其差异如图所示：
![[assets/Pasted image 20240116165219.png]]



# RNN中
层归一化常用在RNN网络中