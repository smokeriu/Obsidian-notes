优化器，是指在反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。

优化器依赖于[梯度下降](梯度下降.md)：
1. 我们首先得到当前参数[梯度](梯度.md)$g_t$：
$$
g_t = \nabla f(w_t)
$$
2. 根据梯度计算一阶动量和二阶动量：
$$
\begin{array}{l}
m_t = \phi(g_1, g_2, \cdots, g_t) \\
\mathbf{V}_t = \sum_{i=0}^{t} x_t^2
\end{array}
$$
这里的
3. 计算当前步的下降梯度：
$$
\eta_t = \frac{\alpha \cdot m_t}{\sqrt{\mathbf{V}_t}}
$$
其中，$\alpha$一般为学习率。

4. 根据计算的下降梯度更新参数：
$$
w_{t+1} = w_t + \eta_{t}
$$

梯度下降衍生了3类算法：
1. 批量梯度下降（BGD）。
2. 随机梯度下降（SGD）。
3. 小批量梯度下降（Mini-batch GD）。
他们的区别主要是一次计算中，选择的样本个数。批量梯度下降每次参数更新会计算所有样本，这导致速度很慢；而**SGD由于每次参数更新仅仅需要计算一个样本的梯度**，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。

事实上由于SGD和BGD的问题，目前一般会使用小批量梯度下降，并且SGD在很多时候可以认为就是小批量梯度下降。其每次参数更新会从所有样本中，选择一个小批量的数据由于参数更新的计算。


在此之上，优化器的发展方向主要有两类：动量优化和自适应学习率。
# 动量优化
动量优化到理念是，**使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡**。

## Momentum
算法思想：参数更新时在一定程度上**保留之前更新的方向**，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过**积累**之前的动量来(_previous_sum_of_gradient_)加速当前的梯度。
其公式为：
$$
\theta = \theta - v_t
$$
其中$v_t$为一个持有方向的变量，为：
$$
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)
$$
其中：
- $\gamma$为momentum的值，为常量
解释：
1. 当梯度$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$为正值，说明处于梯度下降的过程中，则会使得$v_t$越来越大，则更新速度越来越快。
2. 当梯度发生震荡，则此时，$v_{t-1}$仍然为上一次的值，但$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$会变为负值，这使得$v_t$不断变小，直到变回正确的方向。
3. 如此不断逼近局部最优点。

## pyTorch
pytorch的优化器位于torch.optim中。
```python
# model为待优化的模型，这里为小批量梯度下降设置momentum。0.9是一个经验值
opt = torch.optim.SGD(model.parameters(), lr=HP.init_lr, momentum=0.9)

# 训练模型，并使用优化器
x, y = batch
model.zero_grad()
pred = model(x)
loss = criterion(pred, y)
loss.backward()
# 使用优化器，更新相关参数
opt.step()
```

# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。公式为：
$$
\begin{array}{l}
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i} \\
G_{t, i i}=G_{t-1, i i}+g_{t, i}^{2}
\end{array}
$$
其中：
- $g_{t,i}$为第$t$步的梯度。
- $\theta_{t,i}$表示第$t$步的第$i$个参数。

解释：
1. 由于$\mathbf{G_{t,ii}}$为上一步的$\mathbf{G}$加上当前步梯度的平方。这使得其会不断增大。导致$\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}}$越来越小，即学习率越来越小。
2. 这使得随着学习次数的增加，其能在后期避免模型震荡。

另外，由于Ada针对每个参数控制了学习率，这使得其适合稀疏的数据，这也可以更有效的学习需要的数据，（对于稀疏的数据，一些不必要的信息会很快收敛，而不单独控制学习率，会使得一些需要继续学习信息的梯度下降很难快速收敛）

## pyTorch

# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。