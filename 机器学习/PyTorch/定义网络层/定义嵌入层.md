嵌入层的作用是，为一个简单的存储*固定大小的词典*的嵌入向量的查找表。
具体而言，给定一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。即：
- 输入为：一个编号列表。
- 输出为：对应的符号嵌入向量列表。

方法签名：
```python
torch.nn.Embedding(num_embeddings: int, 
				   embedding_dim: int,
				   padding_idx=None,
				   max_norm=None,  
				   norm_type=2.0,   
				   scale_grad_by_freq=False, 
				   sparse=False,  
				   _weight=None)
```
其中：
- `num_embeddings`：词典的大小尺寸，比如总共出现5000个词，那就输入5000。
- `embedding_dim`：嵌入向量的维度，即用多少维来表示一个符号。
- `padding_idx`：填充id。
	- 比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。
- `max_norm`：最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。
- `norm_type`：指定利用什么范数计算，并用于对比max_norm，默认为2范数。
- `scale_grad_by_freq`：根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.
- `sparse`：若为True,则与权重矩阵相关的梯度转变为稀疏张量。

# 用例
假设一个mini-batch如下所示：
```json
['I am a boy.','How are you?','I am very lucky.']
```
显然，这个mini-batch有3个句子，即`batch_size=3`。
我们对句子进行标准化（大写转小写，标点分离等），并统计得到句子的长度：
```python
batch = [['i','am','a','boy','.'],['i','am','very','lucky','.']，['how','are','you','?']]
lens = [5,5,4]
```

之后，为了能够处理，将batch的单词表示转为在词典中的index序号，bk'zd：
```python
batch = [[3,6,5,6,7],[6,4,7,9,5]，[4,5,8,7]]
```

很显然，这个mini-batch中的句子长度*不一致*！所以为了规整的处理，对长度不足的句子，进行填充。填充PAD假设序号是2，填充之后为：
```python
batch = [[3,6,5,6,7,1],[6,4,7,9,5,1],[4,5,8,7,1,2]]
```