当训练迭代期拉长，在某一个迭代期后，学习成果不再能推广到测试数据上，即后续学习无用，则称为**过度拟合**。
换句话说，训练好的模型在训练集上表现比较好，而在测试集的效果就不是那么明显，泛化能力不够。

解决过度拟合最好的办法是增加训练数据集，让其能够推广到更多的数据上，不过不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切实际的选择。
幸运的是，可以使用**规范化**。
其中，最为常见的是[[L2规范化]]。

规范化能够优化过度拟合的原因还不得而知，一种解释是：小的权重在某种程度上，意味着更低的复杂性，也就对数据给出 了一种更简单却更强大解释。

假设神经网络大多数有很小的权重，这最可能出现在规范化的网络中。更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大。这会让规范化网络学习**局部噪声**的影响更加困难。将它看做是一种让单个的证据不会影响网络输出太多的方式。相对的，规范化网络学习去对整个训练集中经常出现的证据进行反应。
对比看， 大权重的网络可能会因为输入的微小改变而产生比较大的行为改变。所以一个无规范化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型。
简言之，规范化网络受限于根据训练数据中常⻅的模式来构造相对简单的模型，而能够抵抗训练数据中的噪声的特性影响。

总而言之，规范化的神经⽹络常常能够⽐⾮规范化的泛化能⼒更强，这只是⼀种**实验事实（empirical fact）**。

# 规范化技术
