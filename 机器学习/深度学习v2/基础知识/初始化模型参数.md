初始化模型参数的用途主要是：
1. 让模型更快收敛。
2. 抑制模型初期就出现梯度消失或梯度爆炸。

常用的初始化模型参数方法有Xavier和Kaiming。

# Xavier

其公式为：
$$
w \sim U(-a, a), a = gain * \sqrt{\frac{6}{fan_{in} + fan_{out}}}
$$
其中：
- w符合`(-a, a)`的均匀分布，也可以使用符合正态分布。
- gain为根据激活函数增益得到的数。
- $fan_{in}$为这一层的input_feature数。$fan_{out}$则是这一层的output_feature数。

一般使用Sigmoid和tanh，会考虑使用Xavier作为这一层的参数初始化。

常用增益如下：

| 激活函数        | gain          |
| --------------- | ------------- |
| Linear/Identity | 1             |
| Conv{1,2,3} D   | 1             |
| Sigmoid         | 1             |
| Tanh            | $\frac{5}{3}$ |
| Relu            | $\sqrt{2}$    |
| Leaky Relu                |       $\sqrt{\frac{2}{1+negtive_slope^2}}$        |


在pytorch中，提供了均匀分布和正态分布两种Xavier初始化的方式：
```python
# 构建层
l1 = nn.Linear(in_features=16, out_features=128)

# 均匀分布
nn.init.xavier_uniform_(l1.weight, gain=nn.init.calculate_gain('sigmoid'))

# 正态分布
nn.init.xavier_normal_(l1.weight, gain=nn.init.calculate_gain('sigmoid'))
```



# Kaiming
其公式为：
$$
w \sim U(-bound, bound), bound=\sqrt{\frac{6}{(1+a^2) + fan_{in}}}
$$
其中：
- w符合`(-bound, bound)`的均匀分布，也可以使用符合正态分布。
- a是`leaky_relu`的负半轴斜率。
- $fan_{in}$为这一层的input_feature数。

一般使用relu相关的激活函数，会考虑使用Kaiming作为这一层的参数初始化。



# 参考

[4.8. 数值稳定性和模型初始化-Xavier](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier)