与[[梯度下降#代价方程]]中使用的二次代价方程相比，交叉熵代价函数能够解决**学习缓慢**的问题。
> 二次代价方程，在接近错误和接近正确时，其学习较为缓慢。在中间则学习较快，图形参见[[感知器和神经元#S型神经元的定义]]。数学原因参考[[交叉熵代价函数#二次代价函数为何会学习缓慢]]。
# 定义
交叉熵代价函数的定义方程为：
$$
C = - \frac{1}{n} 
\sum_x 
[
y \ln{a} + (1-y) \ln{(1-a)}
]
$$
其中：
- $n$为训练样本数。
- $x$为训练的一个样本。
- $y$为训练对应的输出。
- $a$为目标输出。
# 特点
## 非负
公式中，求和中的所有独立的项都是负数的，因为对数函数的定义域是$(0,1)$。而$ln{x} < 0, x \in (0,1)$。
> 定义域参见[[感知器和神经元#S型神经元的定义]]。
## 实际输出接近目标值时，交叉熵接近0
对目标输出进行偏导：
$$
\frac{\partial{C}}{\partial{a}}
=
- \frac{y}{a} + \frac{1-y}{1-a}
=
\frac{a-y}{(1-a)a}
$$
> 注意C的公式前方有个负号。
## 避免学习下降
利用[[../../../数学/基础/微积分/链式法则|链式法则]]，对权重进行偏导
$$
\frac{\partial{C}}{\partial{w_j}}
=
-\frac{1}{n} 
\sum_x (
\frac{y}{\sigma(z)} - \frac{(1-y)}{1-\sigma(z)}
)
\frac{\partial{\sigma}}{\partial{w_j}}
=
\frac{1}{n}
\sum_x x_j(\sigma(z)-y)
$$
> 其中，$\frac{\partial{\sigma}}{\partial{w_j}}$可以被写作$\sigma^{'}(z)x_j$。

所以，权重学习的速度，受$\sigma(z)-y$ 影响，即误差越大，速度越快。

通过同样的手段，可以得到偏置的偏导数：
$$
\frac{\partial{C}}{\partial{b}}
=
\frac{1}{n}
\sum_x (\sigma(z)-y)
$$
> 对偏置的偏导数与权重基本一致。

# 什么是交叉熵
之所以得到交叉熵的公式，其实是反推流程，即我们希望对权重或偏置的导数只与$a-y$有关，这样，结果越偏离正确值，则代价越大。通过一些数学计算和积分，可以得到交叉熵代价函数的定义。
在信息论中，交叉熵是“不确定性”的一种度量。



# 二次代价函数为何会学习缓慢
我们[[../../../数学/基础/微积分/链式法则|链式法则]]对[[反向传播#代价方程]]进行偏导，公式过程与[[交叉熵代价函数#避免学习下降]]中类似，得到：
$$
\frac{\partial{C}}{\partial{w_{jk}^L}}
=
-\frac{1}{n}
\sum_x
a_k^{L-1}(a_j^L - y_j)\sigma^{'}(z_j^L)
$$
这里，其学习效率受到上一层的激活值$a_k^{L-1}$和$\sigma^{'}(z_j^L)$影响，主要是后者，导致了学习缓慢。