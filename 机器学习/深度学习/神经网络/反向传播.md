利用反向传播可以计算梯度，在讨论反向传播前，我们先重新约定神经网络的一些符号。
# 使用矩阵快速计算输出

在讨论反向传播前，我们先重新约定神经网络的一些符号。
使用$w_{jk}^l$来表示从`(l-1)`层的第k个神经元，到`l`层的第`j`个神经元的权重，例如，$w_{24}^3$表示第2层的第4个神经元到第3层的第二个神经元的**权重**。
使用$b_{j}^l$表示第`l`层的第`j`个的**偏置**。
使用`a_{j}^l`表示第`l`层的第`j`个的**激活值**。即神经网络的计算输出。
用图像表示如下：
![[assets/Pasted image 20230727175123.png|500]]

通过上述定义，我们重新利用[[感知器和神经元#S型神经元的定义]]，中对输出进行定义：
$$
a_{j}^l = \sigma(\sum_{k} w_{jk}^l a_{k}^{l-1} + b_{j}^l)
$$
> 其中，$\sigma$为S型函数

## 带权输入
我们为第`l`层定义一个权重矩阵$w^l$，其为连接到第`l`层各个神经元的权重组成的矩阵。
> 例如，在第`j`行的第`k`列元素是$w_{jk}^l$。

对于偏置的第`l`层，也如此定义一个向量$b^l$。第`x`个元素，这一层表示第`x`个的偏置。
对于激活值也如此定义为向量$a^l$。
则我们将上述公式重写为：
$$
a^l = \sigma (w^l a^{l-1} + b^l)
$$

同时，我们称$z^l \equiv w^l a^{l-1} + b^l$为`l`层神经元的**带权输入**，所以上述公式写作：
$$
a^l = \sigma (z^l)
$$
同样，$z^l$也是由$z_{j}^{l}$组成的向量，$z_{j}^{l}$表示的是第`l`层的第`j`个神经元的激活函数的带权输入。其可以写为：
$$
z_{j}^{l} = \sum_j w_{jk}^{l} a_k^{l-1} + b_j^l
$$


# 假设和背景知识
根据上面的定义，重写[[梯度下降#代价方程]]：
$$
\symbfit{C}
\equiv 
\frac{1}{2n} \sum_{x}{}\vert\vert y(x) -a^L(x) \vert\vert ^2
$$
其中
- n是训练样本总数。
- $y(x)$为输入`x`时对应的输出。
- $L$表示网络的层数。
- $a^L(x)$是当输入为x时，网络输出的**激活值向量**。
基于上述公式提出两个假设：
## 第一个假设
代价函数可以被写成一个在每个训练样本$x$上的代价函数$C_x$的均值$C = \frac{1}{n} \sum_x C_x$。对于每个独立的训练样本，其代价为$C_x = \frac{1}{2} \vert\vert y - a^L \vert\vert ^2$。

## 第二个假设
将代价写作神经网络输出的函数：
$$
cost C = C(a^L)
$$
可以进行这个假设的原因是，我们的输入是固定的，其对应的(目标)输出也是固定的，我们可以把最终的代价看做至于**输出的激活值**有关的函数。

## Hadamard乘积
假设$\vec{s}$和$\vec{t}$是两个同样维度的向量，则使用$\vec{s} \odot \vec{t}$表示按元素乘积，例如：
$$
\left[
	\begin{array}{c}
	1 \\
	2 \\
	\end{array}
\right]
\odot
\left[
	\begin{array}{c}
	3 \\
	4 \\
	\end{array}
\right]
=
\left[
	\begin{array}{c}
	1*3 \\
	2*4 \\
	\end{array}
\right]
=
\left[
	\begin{array}{c}
	3 \\
	8 \\
	\end{array}
\right]
$$
## 四个基本方程
我们引入一个中间量$\delta_{j}^{l}$，称其为第`l`层的第`j`个神经元的**误差**。它的意义在于，当这个神经元接受输入时，输出为$\sigma(z_{j}^{l})$，我们假设有一个变化，导致其输出变为了$\sigma(z_{j}^{l} + \Delta{z_{j}^{l}})$。则整体的变化为$\frac{\partial{C}}{\partial{z_{j}^{l}}} \Delta{z_{j}^{l}}$。据此，我们定义一个误差：
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
$$

同时，我们使用$\delta^l$来表示第`l`层的误差向量。
### 输出层误差方程
$$
\delta_{j}^L = 
\frac{\partial{C}}{\partial{a_{j}^{L}}} \sigma^{'}(z_{j}^L)
$$
#### 证明
根据定义$z^l \equiv w^l a^{l-1} + b^l$，$a^l = \sigma (z^l)$，可以将a看做是z的函数，是根据链式法则，可得
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
=
\sum
\frac{\partial{C}}{\partial{a_{k}^{L}}} 
\frac{\partial{a_{k}^{L}}}{\partial{z_{j}^{L}}}
=
\frac{\partial{C}}{\partial{a_{j}^{L}}} \sigma^{'}(z_{j}^L)
$$

转换为向量，根据Hadamard乘积，则为：
$$
\delta^L = \nabla_a C \odot \sigma^{'} (z^L)
$$
> $\nabla_a C$为一个向量，每个元素为$\frac{\partial{C}}{\partial{a_{j}^{L}}}$。

### 使用下一层误差表示当前层
$$
\delta^l = ( (w^{l+1})^T \delta^{l+1} ) 
\odot
\sigma^{'}(z^l)
$$
> $w^{l+1}$是第`l+1`层的权重矩阵，对其转置，意味着反向移动误差。通过这个方程，我们可以将误差不断向输入传递。

#### 证明
根据[多变量下的链式法则](链式法则.md#多变量下的链式法则)量：
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
=
\sum_k 
\frac{\partial{C}}{\partial{z_{k}^{l+1}}} 
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
= \sum_k 
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
\delta_{k}^{l+1}
$$
> 注意，z的下标是k，因为这里z是第`l+1`层。这里主要是和`j`区分开来。

对于具体的z，根据[[反向传播#带权输入]]的定义，则是：
$$
z_k^{l+1} 
= \sum_j w_{kj}^{l+1} a_j^l + b_k^{l+1}
= \sum_j w_{kj}^{l+1} \sigma(z_j^l) + b_k^{l+1}
$$
对$z_k^{l+1}$进行微分，得到
$$
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
=
w_{kj}^{l+1} \sigma^{'}(z_j^l)
$$
带入链式法则的结果，可得
$$
\delta_{j}^l = \sum_k w_{kj}^{l+1} \delta_{k}^{l+1} \sigma^{'}(z_j^l)
$$
将其转换为向量形式即可。
> 注意，权重$w$的定义中，k和j交换了位置。而$\delta$的下标则依旧是k，两者转换为矩阵时，需要权重矩阵进行转置。

### 代价函数关于网络中任意偏置的改变率
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{b_{j}^{l}}}
$$
即误差和偏导偏置的uu