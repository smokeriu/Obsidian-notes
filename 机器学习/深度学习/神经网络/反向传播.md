利用反向传播可以计算梯度，在讨论反向传播前，我们先重新约定神经网络的一些符号。
# 使用矩阵快速计算输出

在讨论反向传播前，我们先重新约定神经网络的一些符号。
使用$w_{jk}^l$来表示从`(l-1)`层的第k个神经元，到`l`层的第`j`个神经元的权重，例如，$w_{24}^3$表示第2层的第4个神经元到第3层的第二个神经元的**权重**。
使用$b_{j}^l$表示第`l`层的第`j`个的**偏置**。
使用`a_{j}^l`表示第`l`层的第`j`个的**激活值**。
用图像表示如下：
![[assets/Pasted image 20230727175123.png|500]]

通过上述定义，我们重新利用[[感知器和神经元#S型神经元的定义]]，中对输出进行定义：
$$
a_{j}^l = \sigma(\sum_{k} w_{jk}^l a_{j}^{l-1} + b_{j}^l)
$$
> 其中，$\sigma$为S型函数

我们为第`l`层定义一个权重矩阵$w^l$，其为连接到第`l`层各个神经元的权重组成的矩阵。
> 其中第`x`列为第`l`层的第`x`个神经元，第`y`行表示为第`l-1`层的第`y`个神经元。

对于偏置的第`l`层，也如此定义一个向量$b^l$。第`x`个元素，这一层表示第`x`个的偏置。
对于激活值也如此定义为向量$a^l$。
则我们将上述公式重写为：
$$
a^l = \sigma (w^l a^{l-1} + b^l)
$$

同时，我们称$z^l \equiv w^l a^{l-1} + b^l$为`l`层神经元的**带权输入**，所以上述公式写作：
$$
a^l = \sigma (z^l)
$$
同样，$z^l$也是由$z_{j}^{l}$组成的向量，$z_{j}^{l}$表示的是第`l`层的第`j`个神经元的激活函数的带权输入。

# 假设
根据上面的定义，重写[[梯度下降#代价方程]]：
$$
\symbfit{C}
\equiv 
\frac{1}{2n} \sum_{x}{}\vert\vert y(x) -a^L(x) \vert\vert ^2
$$
其中
- n是训练样本总数。
- $y(x)$为输入`x`时对应的输出。
- $L$表示网络的层数。
- $a^L(x)$是当输入为x时，网络输出的**激活值向量**。
基于上述公式提出两个假设：
1. 代价函数可以被写成一个在每个训练样本$x$上的代价hj