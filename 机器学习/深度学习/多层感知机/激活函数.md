> 激活函数一定是非线性的

# ReLU函数
因为激活函数一定是非线性的，ReLU的思路就是用最简单的方式将线性函数转换为非线性：
$$
\operatorname{ReLU}(x) = \max(x, 0).
$$
其图像如下：
![](Pasted%20image%2020230806165744.png|500)
即，我将$<0$的内容丢弃，那么原有的函数就不再是线性函数。ReLU会使得$x=0$时函数不可导，所以对于这种情况，一般约定使用$x<0$的导数，即：
$$
\operatorname{ReLU^{'}}(x) =
\left\{
	\begin{aligned} 
		 & 1, x>0  \\ 
		& 0, x \leq 0
	\end{aligned} 
\right.
$$
使用ReLU的原因是：
- 它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好。
- ReLU减轻了困扰以往神经网络的**梯度消失问题**。

## pReLU函数
是ReLU函数的变体：
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).
$$


# Sigmoid函数
# tanh函数
