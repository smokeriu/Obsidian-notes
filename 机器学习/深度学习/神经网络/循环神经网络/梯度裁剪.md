对于长度为$T$的序列，我们计算梯度时，发生在遍历完后，此时会产生$\mathcal{O}(T)$的矩阵**乘法**链，正如[[../../多层感知机/不稳定的梯度|不稳定的梯度]]中提到的一样，过长的$T$可能导致数值不稳定， 例如可能导致梯度爆炸或梯度消失。

假定在向量形式的$\mathbf{x}$中， 或者在小批量数据的负梯度$\mathbf{g}$方向上。使用常数$\eta > 0$作为学习率，则在一次迭代中，$\mathbf{x} \gets \mathbf{x} - \eta \mathbf{g}$。我们假设目标函数表现良好——函数$f$在常数$L$上是*利普希茨连续*的。即：
$$
|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|
$$
特别的，有：
$$
|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|
$$

这意味着我们不会观察到超过$L \eta\|\mathbf{g}\|$的变化。 这既是坏事也是好事。 
- 坏的方面，它限制了取得进展的速度； 
- 好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。

梯度过大时，从而优化算法可能无法收敛。我们可以降低$\eta$来解决。而对于更加常见的，**梯度过小**的情况，一个通用的办法是，将梯度$\mathbf{g}$投影回给定半径 （例如$\theta$）的球来裁剪梯度$\mathbf{g}$：
$$
\mathbf{g} \gets \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
$$

