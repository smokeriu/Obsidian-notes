当我们再训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者值训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候我们就需要使用这个函数分离出来，避免反向传播。

detach会返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是`requires_grad`为false，得到的这个`tensor`永远不需要计算其梯度，不具有grad。

这样我们就会继续使用这个新的`tensor`进行计算，后面当我们进行反向传播时，反向传播会在到达这个`tensor`时终止。

需要注意的是：
- 即使我们尝试为新的tensor请求梯度，**它也不会具有梯度grad**。
- 和原始的`tensor`共用一个内存，即一个修改另一个也会跟着改变。