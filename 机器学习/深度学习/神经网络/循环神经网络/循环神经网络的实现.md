# 独热编码
与用于分类的神经网络一样，我们仍然需要通过[[../../通用知识/独热编码|独热编码]]来对数据进行处理，使其对计算机更加友好，也便于后续计算相似度。

简言之，将每个索引映射为相互不同的*单位向量*： 假设词表中不同词元的数目为$N$，则词元的索引范围为0到$N-1$。我们构建一个长度为$N$的全0向量，对于索引为$i$的词元，设置其向量第$i$处值为1。

参考：[[../../../PyTorch/基础/独热编码|PyTorch独热编码]]

# 参数初始化
初始参数化时，有两个主要的参数：
- 词元个数`vocab_size`。
- 隐藏层输出个数`num_hiddens`。
其中，输入和输出的维度是一样的，因为二者的词元取值范围是一样的，由于我们对数据进行了独热编码，则输入和输出的维度数都等于词元个数。
```python
num_inputs = num_outputs = vocab_size
```
接着，我们需要初始化[[循环神经网络#循环层]]的参数：
```python
# normal是一个根据shape生成随机张量的函数。
# 输入权重
# 输入维度是num_inputs，输出维度num_hiddens
W_xh = normal((num_inputs, num_hiddens))
# 隐变量权重。
# 因为隐变量的输入是上一个时间步隐藏层的输出，所以，其输入维度是num_hiddens
W_hh = normal((num_hiddens, num_hiddens))
# 隐藏层偏置，维度与输出维度一致。
b_h = torch.zeros(num_hiddens, device=device)
```
初始化输出层参数：
```python
# 输出层权重
W_hq = normal((num_hiddens, num_outputs))
# 输出层偏置
b_q = torch.zeros(num_outputs, device=device)
```

在Pytorch中，上述参数均需要梯度，故最终的函数为：
```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # 隐藏层参数
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```


## 隐变量初始化
另外，我们一次会计算一个批次的序列，对于每个序列的第一个元素，其隐变量尚未生成。我们需要初始化一个默认的隐变量，其一般通过全0填充，且形状为$批次大小 \times 输出大小$。
```python
def init_rnn_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
```

> 这里使用元组包围，主要是后续其他功能会修改这部分代码。
# 循环神经网络模型
循环神经网络与线性神经网络的最大区别是引入了隐变量，在一次前向传播后，我们需要更新状态，并在后续计算中使用更新的状态：
```python
# state来自于方法init_rnn_state，是一个tuple，这里只要其第一个参数。
H, = state
outputs = []
for X in inputs:
	H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
	Y = torch.mm(H, W_hq) + b_q
    outputs.append(Y)

return torch.cat(outputs, dim=0), (H,)
```

其中，`inputs`为所有训练数据，因为我们训练的是序列数据，所以一个长度为$N$序列会循环$N$次。这里$X$是 $批次大小 \times 单个字符$ 的张量，不过单个字符进行了独热编码。
1. 先计算第一个字符和之前的$H_0$计算得到循环层的输出$H_1$。
2. 使用$H_1$计算输出层的输出$Y_1$。
3. 计算第二个字符$2$与$H_1$，得到循环层的输出$H_2$。并进一步得到$Y_2$
4. 利用循环，不断更新$H_t$，并得到最终输出组合形成的$Y$列表。
	1. 因为我们输入的是长度为$N$的序列，所以会产生$N-1$个输出。
