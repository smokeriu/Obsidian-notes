一般而言，因为损失函数在最上面，数据在最下面，在反向传播时，靠近损失函数的层训练得更快，然而，底部的层不仅仅训练得慢，而且其一旦变动，顶部的层也会跟着变，这导致顶层被重复训练，最终导致收敛慢。
批量归一化主要用于解决这一问题，即学习底部层的时候，避免变化顶部层。

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：
- 在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。
- 应用比例系数和比例偏移。