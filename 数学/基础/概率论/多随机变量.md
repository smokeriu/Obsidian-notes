很多时候，我们会考虑多个随机变量。
# 联合概率
联合概率，即两个事件同时发生的概率，记为：
$$
P(A, B)
$$
> 也可以记为$P(AB)$或$P(A \cap B)$。

例如，$P(A=a,B=b)$表示$A=a$和$B=b$这两个事件同时发生的概率。
# 条件概率
在联合概率之上，有如下结论：
$$
0 \leq \frac{P(AB)}{P(A)} \leq 1
$$
我们称这个比率为条件概率。特别的，记为：
$$
P(B=b \mid A=a)
$$
表示：其为事件A发生下，事件B的概率，

# 贝叶斯定理
根据乘法原则和条件概率，有$P(A,B) = P(B \mid A)P(A)$，根据对称性，可得$P(A,B) = P(A \mid B)P(B)$，因为$P(B)>0$，可得：
$$
P(A \mid B) = \frac{ P(B\mid A)P(A) }{P(B)}
$$
# 边际化
即对时间概率**求和**。有：
$$
P(B) = \sum_A P(A,B)
$$
即B的概率，相当于计算A的所有可能得选择，并将所有选择的联合概率聚合在一起。

上述称为边际化，边际化结果的概率或分布称为**边际概率**。
# 独立性
如果两个随机变量$A$和$B$是独立的，意味着事件$A$的发生跟事件$B$的发生无关。记为：
$$
A \perp B
$$
根据贝叶斯定理，马上就能同样得到$P(A\mid B)=P(A)$。根据公式，进一步有$P(AB)=P(A) \cdot P(B)$，意味着**联合概率等于边缘概率的乘积**。
而对于非独立的情况，我们称事件A和事件B是**依赖**的。

引入随机变量C，当且仅当$P(A,B\mid C) = P(A \mid C) P(B \mid C)$时，两个随机变量A和B是**条件独立**的。这个情况表示为：
$$
A \perp B \mid C
$$
也就是说，当C发生时，A发生与否与B发生与否是无关的。即事件C发生是**先验条件**。他表示：
- 事件C的发生，使本来**依赖**的事件A和事件B**独立**起来。

# 马尔科夫链
假设我们有事件$A,B,C$，其中，事件C只依赖于事件B，事件B只依赖于事件A，则他们构成了马尔科夫链，则有如下公式：
$$
P(ABC) = P(C|AB)P(AB)
=
P(C|B)P(B|A)P(A)
$$
> 因为事件C只依赖于事件B，则$P(C|AB) = P(C|B)$。