一般而言，因为损失函数在最上面，数据在最下面，在反向传播时，靠近损失函数的层训练得更快，然而，底部的层不仅仅训练得慢，而且其一旦变动，顶部的层也会跟着变，这导致顶层被重复训练，最终导致收敛慢。

批量归一化主要用于解决这一问题，即学习底部层的时候，避免变化顶部层。