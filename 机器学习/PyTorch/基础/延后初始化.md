大部分框架提供了*延后初始化*，即直到数据第一次通过模型传递时，框架才会*动态*地推断出每个层的大小。如此的好处是，由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。

> 在Pytorch中，延迟初始化仍然是一个开发中的新特性，其可能在未来发生大的改动。
> Pytorch将延迟初始化的相关特性，由类`nn.modules.lazy.LazyModuleMixin`来控制，相关的代码也在`nn.modules.lazy`下。

Torch中，[定义线性层](定义线性层.md)提供了它的延迟版本，`nn.LazyLinear`：
```python
# nn.LazyLinear
torch.nn.LazyLinear(out_features: int, ...)
```

与通常版的相比，其不需要设定输入的features数量：
```python
# nn.Linear
torch.nn.Linear(in_features: int, out_features: int, ... )
```


由于其未定义输入的特征维度数量，所以，它的参数都是未被初始化的：
```python
net[0].weight
# <UninitializedParameter>
```

仅当我们第一次传入特征张量后，他才会初始化相关的参数：
```python
net(X)

net[0].weight.shape
# torch.Size([256, 20])
```

特别的，一旦参数完成初始化后，我们就只能按照这个参数使用net，下面的使用将会报错：
```python
X1 = torch.rand([2,2])
X2 = torch.rand([2,2])

net(X1)
net(X2) # exception
```

# 延迟初始化的参数初始化
对于延迟初始化，如果想要使用[自定义参数初始化](参数管理.md#参数初始化)，则需要在其传递了第一次参数后使用：
```python
net1(X)
net1.apply(init) # def init(m):
```