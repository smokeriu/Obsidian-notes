一般而言，因为损失函数在最上面，数据在最下面，在反向传播时，靠近损失函数的层训练得更快，然而，底部的层不仅仅训练得慢，而且其一旦变动，顶部的层也会跟着变，这导致顶层被重复训练，最终导致收敛慢。
批量归一化主要用于解决这一问题，即学习底部层的时候，避免变化顶部层。

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：
- 在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。
- 应用比例系数和比例偏移。
正是由于这个基于*批量*统计的标准化，才有了*批量规范化*的名称。

> 请注意，如果我们尝试使用**大小为1**的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用**足够大的小批量**，批量规范化这种方法才是有效且稳定的。因此，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。

