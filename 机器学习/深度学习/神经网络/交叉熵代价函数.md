与[[梯度下降#代价方程]]中使用的二次代价方程相比，交叉熵代价函数能够解决**学习缓慢**的问题。
> 二次代价方程，在接近错误和接近正确时，其学习较为缓慢。在中间则学习较快，参见[[感知器和神经元#S型神经元的定义]]。
# 定义
交叉熵代价函数的定义方程为：
$$
C = - \frac{1}{n} 
\sum_x 
[
y \ln{a} + (1-y) \ln{(1-a)}
]
$$
其中：
- $n$为训练样本数。
- $x$为训练的一个样本。
- $y$为训练对应的输出。
- $a$为目标输出。
# 特点
## 非负
公式中，求和中的所有独立的项都是负数的，因为对数函数的定义域是$(0,1)$。而$ln{x} < 0, x \in (0,1)$。
> 定义域参见[[感知器和神经元#S型神经元的定义]]。
## 实际输出接近目标值时，交叉熵接近0
对目标输出进行偏导：
$$
\frac{\partial{C}}{\partial{a}}
=
- \frac{y}{a} + \frac{1-y}{1-a}
=
\frac{a-y}{(1-a)a}
$$
> 注意C的公式前方有个负号。
## 避免学习下降
利用[[../../../数学/基础/微积分/链式法则|链式法则]]，对权重进行偏导
$$
\frac{\partial{C}}{\partial{w_j}}
=
-\frac{1}{n} 
\sum_x (
\frac{y}{\sigma(z)} - \frac{(1-y)}{1-\sigma(z)}
)
\frac{\partial{\sigma}}{\partial{w_j}}
=
\frac{1}{n}
\sum_x x_j(\sigma(z)-y)
$$
其中，$\frac{\partial{\sigma}}{\partial{w_j}}$可以被写作$\sigma^{'}(z)x_j$，