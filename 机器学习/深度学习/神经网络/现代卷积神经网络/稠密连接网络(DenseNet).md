> 是[[残差网络(ResNet)]]和[[../../../../数学/基础/微积分/泰勒公式|泰勒公式]]的结合。

在ResNet中，我们函数$f$拆分为两部分：一个简单的线性项$x$和一个复杂的非线性项$g(x)$。不过二者是简单相加的。
DenseNet中，我们通过*连接*来替代简单相加：
$$
\mathbf{x} \to \left[
\mathbf{x},
f_1(\mathbf{x}),
f_2([\mathbf{x}, f_1(\mathbf{x})]), f_3([\mathbf{x}, f_1(\mathbf{x}), f_2([\mathbf{x}, f_1(\mathbf{x})])]), \ldots\right].
$$
即他们的关系变为了：
![[assets/Pasted image 20230828164704.png|500]]
> 上图中，左侧围ResNet，右侧为DenseNet。

整体上，DenseNet由*稠密块*(Dense block)和*过渡层*(transition layer)组成。前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。

# 稠密块
稠密块即上文所说的，由多个卷积块组成，每个卷积块使用*相同数量的输出通道*。在前向传播中，我们将每个卷积块的输入和输出在**通道维**上连结。这使得：
1. 输入和输出在其他维上需要保持一致，对于卷积块，则一般需要保证**不能修改高宽的大小**。

显然，随着稠密块的加深，输出通道呈现明显的增长。例如输入通道数为3，我们假定卷积块的输出通道设置为10，则对于由两个卷积块组成的稠密块，其最终的输出通道数为23。
```python
blk = DenseBlock(2, 3, 10) # 卷积块数、输入通道数、卷积块输出通道数。
X = torch.randn(4, 3, 8, 8)
Y = blk(X)
Y.shape
# Size([4, 23, 8, 8])
```
特别的，输出通道数相比于输入通道数的增长，我们称为增长率。
# 过渡层
由于每个稠密块都会带来通道数的增加，使用过多则会过于复杂化模型。 而过渡层可以用来控制模型复杂度。 它通过[[../卷积神经网络/1x1卷积层|1x1卷积层]]来减小通道数，并使用步幅为2的[[hv减半高和宽，从而进一步降低模型复杂度。