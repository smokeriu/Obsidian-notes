考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。 每一层$l$由变换$f_l$定义， 该变换的参数为权重$\mathbf{W}^{(l)}$， 其隐藏变量是$\mathbf{h}^{(l)}$（特别的，令$\mathbf{h}^{(0)} = \mathbf{x}$）。
> 所谓隐藏变量，其实指的是隐藏层的输出。

则该网络表示为：
$$
\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) , \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).
$$
> 没有找到关于符号$\circ$的准确定义，但不影响这个公式的理解。

如果隐藏变量和输入都是向量，则输出对于任意权重矩阵的梯度，有：
$$
\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.
$$

^1cb2df

换言之，该梯度是$L-l$个矩阵$\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$与梯度向量$\mathbf{v}^{(l)}$的乘积。

> 这里本质是根据[[../../../数学/基础/微积分/链式法则|链式法则]]求导。不过我们将最后一项看成$\mathbf{v}^{(l)}$。但没有考虑激活函数的影响。

因此，我们容易受到数值下溢问题的影响。如果切换到对数空间，数值表示的压力从尾数转移到指数，但问题依旧存在，矩阵可能具有各种各样的特征值。 他们可能很小，也可能很大； 他们的乘积可能非常大，也可能非常小。

因此，面临了如下的问题：
- 梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。
- 梯度爆炸：参数更新过大，破坏了模型的稳定收敛。

# 梯度消失
梯度消失最常见的例子就是使用[[激活函数#Sigmoid函数|Sigmoid函数]]作为激活函数，这是因为当取值过大或过小时，sigmoid函数的导数的值趋近于0，这也就导致提取趋近于0。从而导致梯度消失。
下图中，橘色虚线即为sigmoid的导数图像：
![[assets/Pasted image 20230808193300.png]]

# 梯度爆炸
产生梯度爆炸一般是由于网络过深。根据[[#^1cb2df|每一层的梯度公式]]。如果矩阵中权重大小频繁出现>1的情况，则会导致最终的权重特别的大，如果在这个公式中考虑激活函数，则如果激活函数的导数>1，也会有同样的情况。

> 同样的，如果权重过小，则会出现梯度消失的问题。

# 对称性
> 老实说，没太理解书中的意思，暂略。

# 方案

# 参考
- [详解深度学习中的梯度消失、爆炸原因及其解决方法 - 知乎](https://zhuanlan.zhihu.com/p/33006526)