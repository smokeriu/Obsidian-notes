# 过拟合
如果有足够多的神经元、层数和训练迭代周期， 模型最终可以在*训练集上达到完美的精度*，此时*测试集的准确性却下降了*，这种情况，即模型在训练数据上拟合的比在潜在分布中更接近的现象称为*过拟合*，用于对抗过拟合的技术称为*正则化*。

# 训练误差和泛化误差
*训练误差*（training error）是指， 模型在训练数据集上计算得到的误差。
*泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由**随机选取**的、**未曾在训练集中出现**的数据样本构成。

# 模型复杂性
通常对于神经网络，我们认为需要*更多训练迭代*（epoch）的模型比较复杂， 而需要*早停*（early stopping）的模型（即较少训练迭代周期）就不那么复杂。
我们很难比较本质上不同大类的模型之间（例如，决策树与神经网络）的复杂性。 就目前而言，一条简单的经验法则相当有用： 统计学家认为，能够轻松解释任意事实的模型是复杂的， 而表达能力有限但仍能很好地解释数据的模型可能更有现实用途。
下述wf