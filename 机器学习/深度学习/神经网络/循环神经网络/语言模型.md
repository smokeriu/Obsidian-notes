# 文本序列

文本序列与[序列模型](序列模型.md)类似，例如，想要生成`deep learning is fun`这么一串文字，则可以抽象成概率计算：
$$
P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is})
$$
即，我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 这些概率本质上就是语言模型的参数。

对于常见的单词，想要得到高绿，我们可以计算其在文本中出现的概率：
$$
\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})}
$$
其中：
- $n(x)$表示$x$出现的次数。
- $n(x, x^{'})$表示$x,x^{'}$单词对出现的次数。
显然，这种办法的问题在于，

# N元语法
