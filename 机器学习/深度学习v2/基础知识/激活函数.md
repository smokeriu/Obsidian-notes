> 激活函数一定是非线性的，因为激活函数的目的是避免线性化。

一般而言，神经网络中每一层的输入输出都是一个*线性求和*的过程，下一层的输出只是承接了上一层输入函数的线性变换，激活函数的目的是将线性函数转换为非线性函数。如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。

# ReLU函数
因为激活函数一定是非线性的，ReLU的思路就是用最简单的方式将线性函数转换为非线性：
$$
\operatorname{ReLU}(x) = \max(x, 0).
$$
其图像如下：
![](Pasted%20image%2020230806165744.png|500)
即，我将$<0$的内容丢弃，那么原有的函数就不再是线性函数。ReLU会使得$x=0$时函数不可导，所以对于这种情况，一般约定使用$x<0$的导数，即：
$$
\operatorname{ReLU^{'}}(x) =
\left\{
	\begin{aligned} 
		 & 1, x>0  \\ 
		& 0, x \leq 0
	\end{aligned} 
\right.
$$
使用ReLU的原因是：
- 它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好。
- ReLU减轻了困扰以往神经网络的**梯度消失问题**。

## pReLU函数
是ReLU函数的变体：
$$
\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).
$$
其中：
- $\alpha$是一个线性项，一个可变的标量。用于控制后者的斜率。
这个函数使得$x<0$时，仍然保留了信息，但与$x>0$时不再是线性关系。

## Leaky_Relu
是ReLU函数的变体：
$$
Leaky\_Relu(x) = max(\alpha*x, x) 
$$
其中：
- $\alpha$是一个常量，一般为0.1。
这个函数使得$x<0$时，仍然保留了信息。并且$x>0$时仍然是线性关系。

## Elu
是ReLU函数的变体：
$$
Elu(x) = \left\{
	\begin{aligned} 
		 & x, x \geq 0  \\ 
		& \alpha(e^x-1), x \lt 0
	\end{aligned} 
\right.
$$
其中：
- $\alpha$是一个常量。
# Sigmoid函数
一般称为挤压函数，其公式为：
$$
\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.
$$
其图像为：
![](Pasted%20image%2020230806170613.png)
> 注意，当输入接近0时，sigmoid函数接近线性变换。

Sigmoid函数是一个平滑的、可微的阈值单元近似，这是他的优点。但Sigmoid不再常使用的一个原因在于，指数计算耗费性能，后面介绍的tanh函数同样也有这个问题。

对其求导，可得：
$$
\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).
$$
其图像为：
![](Pasted%20image%2020230806171603.png)

> 注意，$e^{-x}$求导需要利用链式法则，为$-e^{-x}$。

# tanh函数
公式如下：
$$
\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.
$$
图像如下：
![](Pasted%20image%2020230806171706.png)
对其求导，可得：
$$
\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).
$$
图像为：
![](Pasted%20image%2020230806171741.png)

# Maxout
公式为：
$$
Maxout(x) = max(w_1^T x + b_1, w_2^T x+b_2)
$$
Maxout相当于有两个可学习的参数。

# Mish
mish是do ge