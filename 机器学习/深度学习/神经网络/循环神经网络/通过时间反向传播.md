深度学习利用[[../../多层感知机/反向传播|反向传播]]来完成对参数的更新，不过在循环神经网络中，直接使用反向传播，很容易遇到传播链过长的问题，因为$h_t$由$x_t$于$h_{t-1}$生成，这意味着序列末尾元素会反向传播到序列的第一个元素。
在[[循环神经网络]]中，我们知道计算$H_t$的公式为：
$$
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)
$$
其中，$\mathbf{W}_{hh}$是隐变量的权重参数，当我们对其求梯度时，则：
$$
\begin{split}\begin{aligned}\frac{\partial L}{\partial w_h}  & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h}  \\& = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}  \frac{\partial h_t}{\partial w_h}\end{aligned}\end{split}
$$
而对于$\frac{\partial h_t}{\partial w_h}$这部分，由于$H_t$和$H_{t-1}$的计算都依赖于$W_{hh}$，则：
$$
\frac{\partial h_t}{\partial w_h}= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}
$$
进一步推广到$H_1$，则有：
$$
\frac{\partial h_t}{\partial w_h}=\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}
$$
可以发现，后面这部分是一个连乘带连加的公式，这在实际计算中几乎是不可能实现的，而通过时间反向传播，实际上是循环神经网络中反向传播技术的一个特定应用。

# 截断时间步
比较常见的方案是，*固定*在$\tau$步后截断传播。 这样做导致该模型主要侧重于短期影响，而不是长期影响。 这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。

## 随机截断
进一步，我们可以用一个随机变量替换$\frac{\partial h_t}{\partial w_h}$，这个随机变量是通过使用序列$\xi_t$来实现的， 序列预定义了$0 \leq \pi_t \leq 1$， 其中$P(\xi_t = 0) = 1-\pi_t$且$P(\xi_t = \pi_t^{-1}) = \pi_t$， 因此$E[\xi_t] = 1$。则：
$$
z_t= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}
$$

用图表示这些方法的差异如下：
![[assets/Pasted image 20230912195040.png|500]]
实际上，随机截断的效果并不显得比固定截断要好，所以大部分情况下，会使用更简单的固定截断。

# 反向传播的细节
