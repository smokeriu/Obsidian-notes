在[[泛化和过拟合#模型复杂性]]中提到，模型过于复杂可能会导致过拟合，一个简单的处理途径是限制特征的数量（在多项式回归中体现为限制阶数）。但随着阶数$d$的增长，带有阶数$d$的项数会迅速增加。给定$k$个变量，阶数为$d$的项的个数为${k - 1 + d} \choose {k - 1}$，即$C^{k-1}_{k-1+d} = \frac{(k-1+d)!}{(d)!(k-1)!}$。
因此即使是阶数上的微小变化，比如从2到3，也会显著增加我们模型的复杂性。故而，简单的限制特征数量，会使得模型在过简单或过复杂之间徘徊。

# 范数和权重衰减
*权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。这项技术通过**函数与零的距离**来衡量函数的复杂度， 因为在所有函数$f$中，函数$f=0$（所有输入都得到值0） 在某种意义上是最简单的。

如何衡量函数与零的距离？一个简单的方法是，通过线性函数$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$中的权重向量的某个范数来度量其复杂性，例如$\| \mathbf{w} \|^2$。我们的目的就是使这个范数足够的小，而要保证权重向量比较小， 最常用方法是**将其范数作为惩罚项加到最小化损失**的问题中。这样，优化权重$w$的行为也会加入到学习过程中，

即将[[../神经网络/线性神经网络/损失函数|损失函数]]加上正则化，变更为：
$$
L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$
