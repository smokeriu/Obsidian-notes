如果一个神经元的净输入分布在神经网络中是动态变化的，比如循环神经网络，那么无法应用[[BatchNormalization]]操作。另外批量归一化的效果还取决于小批量值$K$的选择。
层归一化和批归一化不同的是，层归一化是对一个中间层的**所有神经元**进行归一化。这意味着其效果可以不依赖于批量值$K$的选择。

对于第$l$隐藏层，其均值和方差为：
$$
\begin{aligned}&\mu^{(l)}=\frac1{n^{(l)}}\sum_{i=1}^{n^{(l)}}z_i^{(l)}\\\\&\sigma^{(l)^2}=\frac1{n^{(l)}}\sum_{k=1}^{n^{(l)}}(z_i^{(l)}-\mu^{(l)})^2\end{aligned}
$$

其中，$n^{(l)}$为第$l$层的神经元个数。而均值$\mu^{(l)}$与批量归一化的区别在于，批量归一化是在批次角度进行均值计算。对于$K$个样本的一个小批量集合$Z^{(l)}=[z^{(1,l)},\ldots,z^{(k,l)}]$，层归一化是矩阵$Z^{(l)}$对每一列进行归一化，而批量归一化是对每一行进行归一化。其差异如图所示：
![[assets/Pasted image 20240116165219.png]]

> 上图中，C表示channel纬度，一般对应这一层的神经元个数。而N表示Batch_Size。

我们假设输入的矩阵为$(N,H,W,C)$，则批归一化维度为$(N,H,W)$，则输出一个长度为C的向量。而层归一化维度为$(H,W,C)$，输出一个长度为

综上，层归一化可定义为：
$$
\tilde{z}^{(l)}=\frac{z^{(l)}-\mu^{(l)}}{\sqrt{\sigma^{(l)}+\epsilon}}\odot\gamma+\beta\Leftarrow LN_{\gamma,\beta}(z^{(l)})
$$

# RNN中
层归一化常用在RNN网络中，如果我们将批归一化应用于RNN中，则我们需要为序列中的每个时间步计算并存储单独的统计信息（因为第$t$步的均值取决于第$t$步的数据和之前统计的数据）。如果测试序列比任何训练序列都要长，这是有问题的。而层归一化不存在此类问题，因为其归一化项仅取决于**当前时间步对层的求和输入**。

假设在时刻$t$，循环神经网络的隐藏层为$h^t$ ，则对于总输入有公式：
$$
\mathbf{a}^t=W_{hh}h^{t-1}+W_{xh}\mathbf{x}^t
$$
对于这个总输入，则通过归一化可以得到：
$$
\begin{aligned}
& h^t=f[\frac g{\sigma^t}\bigodot(a^t-\mu^t)+b]\mathrm{~} \\
& \mu^t=\frac1H\sum_{i=1}^Ha_i^t\mathrm{~} \\
& \sigma^t=\sqrt{\frac1H\sum_{i=1}^H(a_i^t-\mu^t)^2} \\
\end{aligned}
$$

可以看到，均值和方差仅与当前层的总输入有关，而由于层归一化仅针对神经元进行聚合，故可以处理这种情况。在标准循环神经网络中，循环神经网络的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失。而层归一化的循环神经网络可以有效地缓解这种状况。

# PyTorch
PyTorch预置了LN作为归一化层，参见[[../../../PyTorch/定义网络层/定义层归一化|定义层归一化]]

# 参考
- [论文阅读笔记：Layer Normalization](https://zhuanlan.zhihu.com/p/258977332)
- [模型优化之Layer Normalization](https://zhuanlan.zhihu.com/p/54530247)