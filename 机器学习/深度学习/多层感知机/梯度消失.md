考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。 每一层$l$由变换$f_l$定义， 该变换的参数为权重$\mathbf{W}^{(l)}$， 其隐藏变量是$\mathbf{h}^{(l)}$（特别的，令$\mathbf{h}^{(0)} = \mathbf{x}$）。
> 所谓隐藏变量，其实指的是隐藏层的输出。

则该网络表示为：
$$
\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) , \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).
$$
如果隐藏变量和输入都是向量，则输出对于任意权重矩阵的梯度，有：
$$
\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.
$$
