训练程序中，主要有如下内容：
1. 定期将模型参数保存。
2. 训练模型。

# 模型参数检查点保存和读取
因为涉及到定期保存参数，所以需要一个参数来确定是重新训练还是从头开始训练，使用的是python的`argparse`。
```python
parse = ArgumentParser()  
parse.add_argument(  
    '--c',  
    default=None,  
    type=str,  
    help='train from scratch or resume ',  
)  
  
arg = parse.parse_args()
```
如果从checkpoint读取，则：
```python
if arg.c:  
    checkpoint = torch.load(arg.c)  
    model.load_state_dict(checkpoint['model_state_dict'])  
    opt.load_state_dict(checkpoint['optimizer_state_dict'])  
    start_epoch = checkpoint['epoch']
```
其中，checkpoint为保存的数据，是一个dict。这里主要加载了：
- 模型数据。
- 优化器数据。
- 训练的epoch数。

对应的，其保存函数定义为：
```python
def save_checkpoint(model_: nn.Module, epoch_, opt_: torch.optim.Optimizer, path):  
    save_dict = {  
        'epoch': epoch_,  
        'model_state_dict': model_.state_dict(),  
        'optimizer_state_dict': opt_.state_dict(),  
    }  
    torch.save(save_dict, path)
```

# 训练内容定义
开始训练前，需要如下内容：
- 模型。
- 训练数据。
- 优化器。
- 损失函数。
- 记录到第几个epoch和
则：
```python
# 模型
model = BanknoteClassificationModel().to(HP.device)
# 训练数据
train_dataset = dataset_banknote.BanknoteDataset(HP.trainset_path)
train_loader = DataLoader(train_dataset, 
						  batch_size=HP.batch_size, shuffle=True, drop_last=True)
# 优化器
opt = torch.optim.Adam(model.parameters(), lr=HP.init_lr)
# 损失函数
criterion = nn.CrossEntropyLoss()
```

