# 梯度消失
随着我们增加神经网络的隐藏层数量，我们会发现并没有对学习产生正面影响，这一般是梯度消失引起的。
## 原因
其实主要是用


# 梯度激增
与梯度消失相对应的就是梯度激增，