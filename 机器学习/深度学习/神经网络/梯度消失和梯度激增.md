# 梯度消失
随着我们增加神经网络的隐藏层数量，我们会发现并没有对学习产生正面影响，这一般是梯度消失引起的。
## 原因
其实主要是用[[感知器和神经元#S型神经元]]引起的，对$\sigma$函数求导数，会得到如下图像：
![[assets/Pasted image 20230802162319.png|500]]
特别的，我们对代价函数求某一神经元的偏置的导数，我们会发现导数与$w_j \sigma^{'}$的乘积相关，即当$\vert w_j \vert < 1$时，神经元越靠前，其偏置对最终输出的结果影响越小。

换句话说，随着隐藏层数量的增加，神经元越靠前，其学习越慢。


# 梯度激增
与梯度消失相对应的就是梯度激增，在解释梯度消失的主要原因中，我们限制了$\vert w_j \vert < 1$，而我们修改$w_j$和$b_j$，当$\sigma^{'} = 0.25$，且$\vert w_j \vert$足够大时，就变成了梯度激增。


# 解决
尽管梯度消失和梯度激增是两个问题，并且我们使用了S型神经元来说明，但其本质是——前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。