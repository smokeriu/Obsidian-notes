定义一个块，一般遵循如下的步骤：
1. 接受输入，作为其前向传播函数的参数。
2. 通过前向传播函数来生成输出。
3. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。
	1. 通常这是自动发生的。我们可以利用框架的自动微分。
4. 存储和访问前向传播计算所需的参数。
5. 根据情况，调整参数。

# 创建块

我们的类需要继承`nn.Module`，并提供初始化函数。下面的函数创建了一个多层感知器，并直接初始化了两个线性层：`hidden`和`out`。

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层
```

同时，我们需要定义一个前向传播函数：
```python
# 定义模型的前向传播.
def forward(self, X):
    return self.out(F.relu(self.hidden(X)))
```

至此，我们已经完成了一个最简单的块的定义，它由两个线性层组成，他自身也是一个小的神经网络。

# 顺序块
在Torch中，我们可以利用`nn.Sequential`来将多个块线性顺序连接，我们可以参照自定义块的概念，自定义一个`Sequential`：

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module
```
这里，初始化函数增加了一个变长参数，其有一至多个其他块组成，我们将其保存成一个List。

同时，我们也需要定义一个前向传播函数：
```python
def forward(self, X):
    # OrderedDict保证了按照成员添加的顺序遍历它们
    for block in self._modules.values():
        X = block(X)
    return X
```
这里，我们顺序调用保存的其他块，并返回最终的结果。

值得注意的是，这里使用了`_modules`这个自带的Dict，其是由`nn.Module`定义的，一方面这是一种约定，另一方面，我们在初始化参数时，Module会递归遍历`_modules`，从而实现对所有块的参数初始化。

# 定义前向传播
我们可以在前向传播代码中进行更复杂的定义，例如，我们需要一个计算函数$f(\mathbf{x},\mathbf{w}) = c \cdot \mathbf{w}^\top \mathbf{x}$的层，其中$\mathbf{x}$是输入，$\mathbf{w}$是参数，$c$是某个在优化过程中没有更新的指定常量。

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```
对于前向传播，我们先将线性层作用于输入，将我们的函数