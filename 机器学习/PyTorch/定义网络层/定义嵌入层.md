用于在Pytorch中定义[[../../深度学习/神经网络/通用知识/嵌入层(Embedding)|嵌入层(Embedding)]]。

方法签名：
```python
torch.nn.Embedding(num_embeddings: int, 
				   embedding_dim: int,
				   padding_idx=None,
				   max_norm=None,  
				   norm_type=2.0,   
				   scale_grad_by_freq=False, 
				   sparse=False,  
				   _weight=None)
```
其中：
- `num_embeddings`：词典的大小尺寸，比如总共出现5000个词，那就输入5000。
- `embedding_dim`：特征向量的维度，即用多少维来表示一个符号。
- `padding_idx`：填充id。
	- 比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。
- `max_norm`：最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。
- `norm_type`：指定利用什么范数计算，并用于对比max_norm，默认为2范数。
- `scale_grad_by_freq`：根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.
- `sparse`：若为True,则与权重矩阵相关的梯度转变为稀疏张量。

# 用例
定义一个Long或Int类型的Tensor：
```python
torch.range(0,4,dtype=torch.long)
```
显然，我们拥有如下序列：
`[0,1,2,3,4]`，所以我们的词表大小至少为5，否则会报错。
```python
embed = nn.Embedding(5, 6)
```
第二个参数表示将每个参数转换为6维向量。则：
```
tensor(
	[[ 0.6888, 2.1374, 1.1099, -1.8146, 0.0765, 1.1526], 
	[-0.9453, 0.1902, -0.8554, 1.3648, -1.3557, 1.1961], 
	[ 0.3962, 0.5797, 1.4474, 1.3995, -0.5287, -0.8393], 
	[-1.1349, -1.6222, -1.3150, 0.3496, -1.8176, 0.8105], 
	[-1.3600, 0.5810, -0.8478, -1.1879, 0.4629, 0.8608]], 
	grad_fn=<EmbeddingBackward0>)
```
