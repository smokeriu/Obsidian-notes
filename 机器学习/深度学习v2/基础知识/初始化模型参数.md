初始化模型参数的用途主要是：
1. 让模型更快收敛。
2. 抑制模型初期就出现梯度消失或梯度爆炸。

常用的初始化模型参数方法有Xavier和Kaiming。

# Xavier

其公式为：
$$
w \sim U(-a, a), a = gain * \sqrt{\frac{6}{fan_{in} + fan_{out}}}
$$
其中：
- w符合`(-a, a)`的均匀分布，也可以使用符合正态分布。
- gain为根据激活函数增益得到的数。
- $fan_{in}$为这一层的input_feature数。$fan_{out}$则是这一层的output_feature数。

一般使用Sigmoid和tanh，会考虑使用Xavier作为这一层的参数初始化。

常用增益如下：

| 激活函数        | gain          |
| --------------- | ------------- |
| Linear/Identity | 1             |
| Conv{1,2,3} D   | 1             |
| Sigmoid         | 1             |
| Tanh            | $\frac{5}{3}$ |
| Relu            | $\sqrt{2}$    |
| Leaky Relu                |       $\sqrt{\frac{2}{1+negtive_slope^2}}$        |


在pytorch中：
```python

```
# Kaiming
其公式为：
$$
w \sim U(-bound, bound), bound=\sqrt{\frac{6}{(1+a^2) + fan_{in}}}
$$
其中：
- w符合`(-bound, bound)`的均匀分布，也可以使用符合正态分布。
- a是`leaky_relu`的负半轴斜率。
- $fan_{in}$为这一层的input_feature数。

一般使用relu相关的激活函数，会考虑使用Kaiming作为这一层的参数初始化。