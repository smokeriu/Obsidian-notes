多层感知机是最简单的神经网络，由3种类型的层构成：
- 输入层。
- 输出层。
- 隐藏层。

其用图形表示为：
![[assets/Pasted image 20231114190559.png|500]]


# 概述
## 神经元
每一层由多个神经元构成。
## 输入层
输入层即是对输入的高维抽象。比如说一个单词，可以按照字母抽象成数值，最终拼接形成与单词长度一样的一维向量。每个神经元对应这向量中的一个元素。
## 隐藏层
用于对输入层输出的数据的不同特征进行高维抽象，多层感知机的多层就体现在其可以拥有多个隐藏层。
一般而言，每个神经元负责的特征不同，具体负责哪方面的特征是程序自动洗得的。

> 其与输出层特征抽象后的位置相关。
## 输出层
对输出的数据与预期的标签数据进行映射，输出层的每个神经元需要与标签数据抽象的向量一一对应。
例如在识别数字的模型中，对于十进制的数字，显然是一个十分类的问题，标签根据标签值，将对应坐标的元素设置为1，其与元素设置为0；而训练的输出同样生成一个长度为10的向量，向量中每个元素的值表示着模型认为这个输入是*这个分类的概率*。我们通过损失函数，可以计算这两个向量的差异性，训练的目的就是让程序根据*输入*与*标签*的关系，降低这个差异性，使得最终输出的向量尽可能接近标签对应的向量——向量坐标对应分类的概率值最高。
## 前向传播
一般而言，层在接受到输入后，会经过一个线性变换：
$$
out = f(input) = w * input + b
$$
之后再经过[[../基础知识/激活函数|激活函数]]处理后作为下一层的输入：
$$
input2 = \alpha(output)
$$
当数据到达输出层时，再通过[[../基础知识/损失函数|损失函数]]计算损失值，通过反向传播优化参数，从而训练模型。

## 全连接
多层感知机是全连接的，即每个上游神经元，都会连向下一层每一个神经元。因此，这样的层被称为线性层，多层感知机又叫作：
- 全连接神经网络。
- 稠密连接网络。（Dense）

# PyTorch实现

## 定义数据

```python
data_x = torch.rand(size=[n_item, n_feature])  
data_y = torch.where(torch.subtract(data_x[:, 0] * 0.5, 
					   data_x[:, 1] * 1.5) > 0, 1., 0.).long()  
  
data_y = F.one_hot(data_y)
```

唯一与[[逻辑回归]]不同的地方在于，我们这里考虑使用[[../基础知识/独热编码|独热编码]]，所以要求标签的类型是long。

## 定义网络

网络使用到[[../../PyTorch/定义网络层/自定义块|自定义块]]：将多个Pytorch的[[../../PyTorch/定义网络层/定义线性层|定义线性层]]串联：
```python
class PerceptionModule(nn.Module):  
    def __init__(self, in_feature, activate_method):  
        super(PerceptionModule, self).__init__()  
        self.layer_1 = nn.Linear(in_feature, 8, bias=True)  
        self.layer_2 = nn.Linear(self.layer_1.out_features, 16, bias=True)  
        self.layer_3 = nn.Linear(self.layer_2.out_features, 6, bias=True)  
        self.out = nn.Linear(self.layer_3.out_features, 2, bias=True)  
        self.activate_method = activate_method  
  
    def forward(self, x):  
        o1 = self.activate_method(self.layer_1(x))  
        o2 = self.activate_method(self.layer_2(o1))  
        o3 = self.activate_method(self.layer_3(o2))  
        return self.activate_method(self.out(o3))

model = PerceptionModule(n_feature, F.sigmoid)
```
唯一需要注意的是，下一层的输入特征数，要和上一层的输出特征数一致，且我们定义的输出层输出特征数为2，计算损失时要求标签也是一个长度为2的向量。

## 定义优化器

这里直接使用Pytorch自带的[[../../PyTorch/定义优化器/SGD优化器|SGD优化器]]：
```python
opt = torch.optim.SGD(model.parameters(), lr=0.005)
```
其中：
- `model.parameters()`为上一步定义模型的参数，本质上，就是前向传播过程中的参数信息，如权重、偏置等。
- lr就是学习率。
优化器本质上就是更新参数，所以在前向传播开始前清空梯度：
```python
opt.zero_grad()
# before forward
```
反向传播结束后更新参数：
```python
# after backward
opt.step()
```
## 定义损失函数
这里直接使用Pytorch自带的[[../../PyTorch/定义损失函数/交叉商损失(BCELoss)|交叉商损失(BCELoss)]]作为损失函数：
```python
loss_function = nn.BCELoss()
```

## 训练
```python
for epoch in range(1000):  
    for item in range(n_item):  
        x = data_x[item]  
        y = data_y[item]  
        opt.zero_grad()  
  
        y_hat = model(x.unsqueeze(0))  
        loss = loss_function(y_hat, y.unsqueeze(0).float())  
        loss.backward()  
        opt.step()
```
需要注意的是，