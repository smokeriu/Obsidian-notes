利用反向传播可以计算梯度，在讨论反向传播前，我们先重新约定神经网络的一些符号。
# 使用矩阵快速计算输出

在讨论反向传播前，我们先重新约定神经网络的一些符号。
使用$w_{jk}^l$来表示从`(l-1)`层的第k个神经元，到`l`层的第`j`个神经元的权重，例如，$w_{24}^3$表示第2层的第4个神经元到第3层的第二个神经元的**权重**。
使用$b_{j}^l$表示第`l`层的第`j`个的**偏置**。
使用`a_{j}^l`表示第`l`层的第`j`个的**激活值**。即神经网络的计算输出。
用图像表示如下：
![[assets/Pasted image 20230727175123.png|500]]

通过上述定义，我们重新利用[[感知器和神经元#S型神经元的定义]]，中对输出进行定义：
$$
a_{j}^l = \sigma(\sum_{k} w_{jk}^l a_{k}^{l-1} + b_{j}^l)
$$
> 其中，$\sigma$为S型函数

## 带权输入
我们为第`l`层定义一个权重矩阵$w^l$，其为连接到第`l`层各个神经元的权重组成的矩阵。
> 例如，在第`j`行的第`k`列元素是$w_{jk}^l$。

对于偏置的第`l`层，也如此定义一个向量$b^l$。第`x`个元素，这一层表示第`x`个的偏置。
对于激活值也如此定义为向量$a^l$。
则我们将上述公式重写为：
$$
a^l = \sigma (w^l a^{l-1} + b^l)
$$

同时，我们称$z^l \equiv w^l a^{l-1} + b^l$为`l`层神经元的**带权输入**，所以上述公式写作：
$$
a^l = \sigma (z^l)
$$
同样，$z^l$也是由$z_{j}^{l}$组成的向量，$z_{j}^{l}$表示的是第`l`层的第`j`个神经元的激活函数的带权输入。其可以写为：
$$
z_{j}^{l} = \sum_j w_{jk}^{l} a_k^{l-1} + b_j^l
$$


# 假设和背景知识
## 代价方程
根据上面的定义，重写[[梯度下降#代价方程]]：
$$
\symbfit{C}
\equiv 
\frac{1}{2n} \sum_{x}{}\vert\vert y(x) -a^L(x) \vert\vert ^2
$$
其中
- n是训练样本总数。
- $y(x)$为输入`x`时对应的输出。
- $L$表示网络的层数。
- $a^L(x)$是当输入为x时，网络输出的**激活值向量**。
基于上述公式提出两个假设：
### 第一个假设
代价函数可以被写成一个在每个训练样本$x$上的代价函数$C_x$的均值$C = \frac{1}{n} \sum_x C_x$。对于每个独立的训练样本，其代价为$C_x = \frac{1}{2} \vert\vert y - a^L \vert\vert ^2$。

### 第二个假设
将代价写作神经网络输出的函数：
$$
cost C = C(a^L)
$$
可以进行这个假设的原因是，我们的输入是固定的，其对应的(目标)输出也是固定的，我们可以把最终的代价看做只与**输出的激活值**有关的函数。

## Hadamard乘积
假设$\vec{s}$和$\vec{t}$是两个同样维度的向量，则使用$\vec{s} \odot \vec{t}$表示按元素乘积，例如：
$$
\left[
	\begin{array}{c}
	1 \\
	2 \\
	\end{array}
\right]
\odot
\left[
	\begin{array}{c}
	3 \\
	4 \\
	\end{array}
\right]
=
\left[
	\begin{array}{c}
	1*3 \\
	2*4 \\
	\end{array}
\right]
=
\left[
	\begin{array}{c}
	3 \\
	8 \\
	\end{array}
\right]
$$
## 四个基本方程
我们引入一个中间量$\delta_{j}^{l}$，称其为第`l`层的第`j`个神经元的**误差**。它的意义在于，当这个神经元接受输入时，输出为$\sigma(z_{j}^{l})$，我们假设有一个变化，导致其输出变为了$\sigma(z_{j}^{l} + \Delta{z_{j}^{l}})$。则整体的变化为$\frac{\partial{C}}{\partial{z_{j}^{l}}} \Delta{z_{j}^{l}}$。据此，我们定义一个误差：
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
$$

同时，我们使用$\delta^l$来表示第`l`层的误差向量。
### 输出层误差方程
$$
\delta_{j}^L = 
\frac{\partial{C}}{\partial{a_{j}^{L}}} \sigma^{'}(z_{j}^L)
$$
#### 公式一证明
根据定义$z^l \equiv w^l a^{l-1} + b^l$，$a^l = \sigma (z^l)$，可以将a看做是z的函数，是根据链式法则，可得
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
=
\sum
\frac{\partial{C}}{\partial{a_{k}^{L}}} 
\frac{\partial{a_{k}^{L}}}{\partial{z_{j}^{L}}}
=
\frac{\partial{C}}{\partial{a_{j}^{L}}} \sigma^{'}(z_{j}^L)
$$

转换为向量，根据Hadamard乘积，则为：
$$
\delta^L = \nabla_a C \odot \sigma^{'} (z^L)
$$
> $\nabla_a C$为一个向量，每个元素为$\frac{\partial{C}}{\partial{a_{j}^{L}}}$。

### 使用下一层误差表示当前层
$$
\delta^l = ( (w^{l+1})^T \delta^{l+1} ) 
\odot
\sigma^{'}(z^l)
$$
> $w^{l+1}$是第`l+1`层的权重矩阵，对其转置，意味着反向移动误差。通过这个方程，我们可以将误差不断向输入传递。

#### 公式二证明
根据[多变量下的链式法则](链式法则.md#多变量下的链式法则)：
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{z_{j}^{l}}}
=
\sum_k 
\frac{\partial{C}}{\partial{z_{k}^{l+1}}} 
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
= \sum_k 
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
\delta_{k}^{l+1}
$$
> 注意，z的下标是k，因为这里z是第`l+1`层。这里主要是和`j`区分开来。

对于具体的z，根据[[反向传播#带权输入]]的定义，则是：
$$
z_k^{l+1} 
= \sum_j w_{kj}^{l+1} a_j^l + b_k^{l+1}
= \sum_j w_{kj}^{l+1} \sigma(z_j^l) + b_k^{l+1}
$$
对$z_k^{l+1}$进行微分，得到
$$
\frac{\partial{z_{k}^{l+1}}}{\partial{z_{j}^{l}}}
=
w_{kj}^{l+1} \sigma^{'}(z_j^l)
$$
带入链式法则的结果，可得
$$
\delta_{j}^l = \sum_k w_{kj}^{l+1} \delta_{k}^{l+1} \sigma^{'}(z_j^l)
$$
将其转换为向量形式即可。
> 注意，权重$w$的定义中，k和j交换了位置。而$\delta$的下标则依旧是k，两者转换为矩阵时，需要权重矩阵进行转置。

### 代价函数关于网络中任意偏置的改变率
$$
\delta_{j}^l =  \frac{\partial{C}}{\partial{b_{j}^{l}}}
$$
即误差和偏导偏置的数值一样。
#### 公式三证明
根据[[../../../数学/基础/微积分/链式法则#单变量下的链式法则|单变量下的链式法则]]：
$$
\frac{\partial{C}}{\partial{b_{j}^{l}}}
=
\frac{\partial{C}}{\partial{z_{j}^{l}}}
\frac{\partial{z_{j}^{l}}}{\partial{b_{j}^{l}}}
=
\frac{\partial{C}}{\partial{z_{j}^{l}}}
=
\delta_{j}^l
$$
> 因为z对b求偏导，为常数1。

### 代价函数关于任何一个权重的改变率
$$
\frac{\partial{C}}{\partial{w_{jk}^{l}}}
=
a_k^{l-1} \delta_j^l
$$
> 实际使用时，如果使用矩阵，需要转置$a^{L-1}$。
#### 公式四证明
与[[反向传播#公式三证明]]一致。

# 反向传播算法
反向传播方程给出了一种计算代价函数梯度的方法。
- 输入$x$：为输入层设置对应的激活值$a^1$。
- 前向传播：对每个`l=2,3,...,L`，计算相应的$z^l = w^la^{l-1} + b^l$和$a^l = \sigma(z^l)$。
- 输出层误差$\delta^L$：计算向量$\delta^L = \nabla_a C \odot \sigma^{'} (z^L)$。
	- 参见[[反向传播#公式一证明]]。
- 反向误差传播：对每个`l=L-1,L-2,...,2`，计算$\delta^l = ( (w^{l+1})^T \delta^{l+1} ) \odot \sigma^{'}(z^l)$。
	- 参见[[反向传播#使用下一层误差表示当前层]]。
- 输出：代价函数的梯度由$\delta_{j}^l =  \frac{\partial{C}}{\partial{b_{j}^{l}}}$和$\frac{\partial{C}}{\partial{w_{jk}^{l}}} = a_k^{l-1} \delta_j^l$计算得出
	- 参见[[反向传播#代价函数关于网络中任意偏置的改变率]]
	- 参见[[反向传播#代价函数关于任何一个权重的改变率]]。
简单而言，根据输入，计算出对应的输出，然后反向计算误差，最后得到梯度。

# 代码
1. 通过[[反向传播#带权输入]]，可以记录每一层的z，并计算得到输出。
2. 结合[[反向传播#代价方程]]和[[反向传播#输出层误差方程]]，即可得到输出层的误差。
	1. 因为我们使用的二次误差方程，对$a$求偏导，得到的结果为$a-y$。
		1. a为目标输出，y为实际输出。
3. 通过[[反向传播#代价函数关于任何一个权重的改变率]]和[[反向传播#代价函数关于网络中任意偏置的改变率]]计算得到输出层的偏置和权重的偏导数。
4. 通过[[反向传播#使用下一层误差表示当前层]]得到上一层的误差。并重复第三步。
5. 得到每个神经元的