批量归一化层可以看作是特殊的神经层，加在每一层非线性激活函数之前。因此批量归一化是逐层进行归一化，所以要求效率比较高，因此复杂度比较高的白化方法就不太合适。为了提高归一化效率，一般使用标准归一化，如Z-score标准化。有公式：
$$
\tilde{z}^{(l)}=\frac{z^{(l)}-E(z^{(l)})}{\sqrt{var(z^{(l)}+\epsilon)}}
$$
给定一个包含$K$个样本的小批量样本集合，第$l$层的净输入为$z^{(1,l)},z^{(2,l)},\ldots,z^{(k,l)}$，则得到均值和方差：
$$
\begin{aligned}&\mu_{(l)}=\frac1K\sum_{k=1}^Kz^{(k,l)}\\\\&\sigma_{(l)}^2=\frac1K\sum_{k=1}^K\left(z^{(k,l)}-\mu_{(l)}\right)\odot(z^{(k,l)}-\mu_{(l)})\end{aligned}
$$
其中$\odot$表示[[../../../../数学/基础/向量与图形/Hadamard乘积|Hadamard乘积]]。

由于上述会导致取值集中在0附近，对于[[../激活函数#Sigmoid函数|Sigmoid激活函数]]，这个取值区间刚好接近线性变换区间，减弱了神经网络的非线性性质。而对于[[../激活函数#ReLU函数|ReLU激活函数]]，则会有近半的数据无法使用。为了减弱影响，可以考虑进行平移和缩放：
$$
\tilde{z}^{(l)}=\frac{z^{(l)}-\mu_{(l)}}{\sqrt{\sigma_{(l)}^2+\epsilon}}\odot\gamma+\beta\Leftarrow BN_{\gamma,\beta}(z^{(l)})
$$
其中：
- $\gamma$表示缩放的参数向量。
- $\beta$表示平移的参数向量。

均是一个可以学习的参数。

> 部分框架会把$\gamma$和$\beta$统一为weight和bias。

则对于批量归一化层，简写为：
$$
a^{(l)}=f(BN_{\gamma,\beta}(z^{(l)}))=f(BN_{\gamma,\beta}(Wa^{(l-1)}))
$$
更多信息参见[[../../../深度学习/神经网络/现代卷积神经网络/批量归一化|批量归一化]]
# PyTorch
PyTorch提供了针对不同纬度的数据的批量归一化层。如BatchNorm2d，BatchNorm3d。参见[[../../../PyTorch/定义网络层/定义批量归一化|定义批量归一化]]。
