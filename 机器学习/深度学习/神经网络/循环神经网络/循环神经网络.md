循环神经网络，是一类具备有**隐变量**的神经网络。

# 隐变量
由于序列的长度是不可控的，所以通过[[序列模型#马尔科夫假设]]，我们认为预测当前元素$x_t$，只与前$\tau$个元素有关。但对于之前的单词，也不希望其完全无用，所以和[[序列模型#潜变量模型]]中提及的类似，将之前的信息当做*隐变量*聚合后使用。

对于隐变量$h_t$，其存储了到时间步$t$为止的序列信息，一般而言，我们通过$h_{t-1}$与当前的输入$x_t$来共同更新$h_t$，即：
$$
h_t = f(x_{t}, h_{t-1})
$$


# 隐藏层与隐变量
> 隐变量与隐藏层是不同的事物

在多层感知机中，我们的输出与输入、参数间的关系如下：
$$
\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)
$$
其中，$\mathbf{H}$是隐藏层的输出，$\phi$是激活函数，其会作为输出层的输入：
$$
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q
$$
如果是分类问题，我们可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。

我们在隐藏层中引入隐变量：
$$
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)
$$
其中：
- $\mathbf{H}_{t-1}$为时间步$t-1$的隐变量，且$\mathbf{H}_t \in \mathbb{R}^{n \times h}$。
- $\mathbf{W}_{hh}$为新的权重参数，且$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$。
可以看到，当前时间步的输入与前一个时间步的输出$\mathbf{H}_{t-1}$有关系，具体地说，当前时间步隐藏变量由*当前时间步的输入* 与*前一个时间步的隐藏变量*一起计算得出，可以将其看做是一种状态，故也称为**隐状态**。并且，这是循环的。