---
tags:
  - 损失函数
  - 交叉熵
---
# 用途
PyTorch通过`nn.cross_entropy`来使用交叉熵损失函数，用来判定实际的输出与期望的输出的接近程度。交叉熵的值越小，两个概率分布就越接近。

# 定义
定义函数的主要参数有：
- weight：
	- Tensor, optional。
	- 必须是一个1维的tensor，长度为类别数，每个值对应每一类的权重。
- reduction：
	- 最终的输出类型，默认为`mean`。
	- 可以为`none`或`sum`。

使用函数时需要传入input与target，需要特别注意：
- input：
	- 需要时**原始的**预测输出，*未经过`softmax`或者`normalized`的*。原因是这个函数会首先对输入的原始得分进行`softmax`，所以必须保证输入的是每一类的原始得分。
		- 不能写成`[0.2, 0.36, 0.44]`这种softmax之后的或者`[0, 1, 0]`这种`one-hot`编码。
	- 即输入的形状可以为：
		- `(class_size)`。
		- `(batch_size, class_size)`。
		- `(batch_size, class_size, 其他参数 )`。

- target：
	- 不能是`one-hot`标签，直接输入每个例子对应的类别编号就行了。
	- 比如产生的结果数为`N*C`（N为个数，C为类别数），那么输入的`target`必须输入一个长度为`N`的一维tensor。
	- 即target形状可以为：
		- 

# 用例

```python
loss_fn = nn.CrossEntropyLoss()
# 此处假设batch_size = 1
# 预测2个对象，每个对象分别属于三个类别分别的概率
x_input = torch.randn(2, 3)  
# 这里给出两个对象所属的类别标签即可，此处的意思为第一个对象属于第0类，第二个我对象属于第2类
x_target = torch.tensor([0, 2])  
loss = loss_fn(x_input, x_target)
```

可以看到，target只需要直接使用标签值即可。