柔性最大值的想法其实就是为神经网络定义一种新式的输出层。其也是构建在[[反向传播#带权输入]]定义之上，但不使用S型函数。
# 定义
根据柔性最大值函数的定义，第$j$个神经元的激活值$a_j^L$就是：
$$
a_j^L =
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}}
$$
其中，分母的求和是在所有的输出神经元上进行的。
# 特点
## 所有输出神经元的激活值相加为1
我们可以通过公式证明这一点，且正因如此，当我们增大某一个带权输入，会导致其他输入对应的激活值降低，而增加带权输入对应的激活值增加。
## 输出激活值都是正数
因为指数函数是正的。
## 输出可以被看做 是一个概率分布
柔性最大值层的输出是一些相加为 1 正数的集合。
## 柔性最大值具有单调性
当我们增大某一个带权输入，会导致其他输入对应的激活值降低，而增加带权输入对应的激活值增加。
这意味着，当$j=k$时，$\partial{a_j^L}/\partial{z_k^L} > 0$，反之则$<0$。
### 证明
1. $j=k$时。
$$
\partial{a_j^L}/\partial{z_k^L}
=
\frac{e^{z_j^L}(\sum_k e^{z_k^L})- e^{z_j^L}(\sum_k e^{z_k^L})^{'}}
{(\sum_k e^{z_k^L})^2}
=
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}} - 
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}} 
\cdot
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
$$
由于$j=k$，最后一项可改写为：
$$
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
=
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}}
$$
又有$a_j^L =\frac{e^{z_j^L}}{\sum_k e^{z_k^L}}$，则可以化为：
$$
\partial{a_j^L}/\partial{z_k^L}
=
a_j^L - (a_j^L)^2
$$
因为$a_j^L \in (0,1)$，故$a_j^L - (a_j^L)^2 > 0$。


2. $j \ne k$时。
此时对$z_k^L$求导，会导致$(e^{z_j^L})^{'} = 0$，则：
$$
\partial{a_j^L}/\partial{z_k^L}
=
\frac{0 \cdot (\sum_k e^{z_k^L})- e^{z_j^L}(\sum_k e^{z_k^L})^{'}}
{(\sum_k e^{z_k^L})^2}
=
-
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}} 
\cdot
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
=
-a_j^L \cdot a_k^L
$$
因为$a^L \in (0,1)$，故$-a_j^L \cdot a_k^L < 0$。


## 柔性最大值的非局部性
根据定义，我们可以直到，任何特定的输出激活值$a_j^L$依赖于所有的带权输入。
> 而[[感知器和神经元#S型神经元]]只依赖于自身。

## 逆转柔性最大值层
已知激活值是$a_j^L$，则带权输入为：
$$
z_j^L = \ln a_j^L + C
$$
其中C是与j无关的常量。
### 证明
根据$a_j^L$的定义：$a_j^L =\frac{e^{z_j^L}}{\sum_k e^{z_k^L}}$
有：
$$
\ln a_j^L = 
\ln{e^{z_j^L}} - \ln{\sum_k e^{z_k^L}} =
z_k^L - \ln{\sum_k e^{z_k^L}}
$$
其中，$\ln{\sum_k e^{z_k^L}}$与$j$无关，且是固定的，我们可以简写为$C$，即为原公式。

# 柔性输出如何解决学习缓慢
对于柔性最大值函数，我们需要定义一个新的代价函数，**对数似然**代价函数：
$$
C \equiv  -\ln a_y^L
$$
其中，x为输入，y为对应的目标输出。如果网络的结果与目标结果越接近，则$a_y^L$越接近于0，反之则为负数，且由于对数函数的特性，其yt