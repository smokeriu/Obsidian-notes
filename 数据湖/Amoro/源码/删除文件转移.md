这部分代码，将`eq-delete`数据转换为`pos-delete`数据。

具体的代码位于：
`com.netease.arctic.optimizing.AbstractRewriteFilesExecutor#equalityToPosition`

程序先获取deleteWriter，用于执行实际的删除：
```java
FileWriter<PositionDelete<Record>, DeleteWriteResult> posDeleteWriter = posWriter();
```

然后遍历待删除的数据：
```java
CloseableIterator<Record> iterator = dataReader.readDeletedData().iterator()
```

# readDeletedData
这里`readDeletedData`获取的是[[构建OptimizingPlan]]中生成的`rePosDeletedDataFiles`，`readOnlyDeleteFiles`和`rewrittenDeleteFiles`组成。并与`posDeleteList`一起构建成BasicArcticFileScanTask。

总而言之，这里构建了`DataFile`与`eq-delete`之间的映射。

```java
List<PrimaryKeyedFile> equlityDeleteList = input.equalityDeleteForMixed();
List<PrimaryKeyedFile> dataFiles = input.rePosDeletedDataFilesForMixed();
allTaskFiles.addAll(equlityDeleteList);  
allTaskFiles.addAll(dataFiles);

allTaskFiles.stream()  
    .map(file -> new BasicArcticFileScanTask(file, 
					    posDeleteList, table.spec()));
```

在后续的readDeletedData方法中，系统读取对应的DataFile数据，再分别读取eq-delete中和pos-delete中的数据，并构建过滤。

过滤：
- `eq-delete`主要是通过deletes的数据，确定数据中，被删除数据的key。
- `pos-delete`则直接获取的删除数据所在的文件和位置。
```java
public CloseableIterable<StructForDelete<T>> filterNegate(  
    CloseableIterable<StructForDelete<T>> records) {  
  Predicate<StructForDelete<T>> inEq = applyEqDeletes();  
  Predicate<StructForDelete<T>> inPos = applyPosDeletes();  
  Predicate<StructForDelete<T>> or = inEq.or(inPos);  
  Filter<StructForDelete<T>> remainingRowsFilter =  
      new Filter<StructForDelete<T>>() {  
        @Override  
        protected boolean shouldKeep(StructForDelete<T> item) {  
          return or.test(item);  
        }  
      };  
  
  return remainingRowsFilter.filter(records);  
}
```
通过上述两种数据，再读取对应的parquet文件，从而获取带删除的数据，并构建成Record。


# 遍历
这里主要是拿到数据所在的文件及其中的位置：
```java
Record record = iterator.next();  
// 删除数据位于的原始文件
String filePath = (String) record.getField(MetadataColumns.FILE_PATH.name()); 
// 在原始文件中的位置
Long rowPosition = (Long) record.getField(MetadataColumns.ROW_POSITION.name());

positionDelete.set(filePath, rowPosition, null);
```

> 在大部分Lakehouse中，删除/更新的数据，会作为记录存储在一个文件中，并在读取时再与实际数据合并，达到删除的效果。

在得到数据记录的位置后，amoro会使用具体的`FileWriter`对其进行删除，具体有两类实现：
`ArcticTreeNodePosDeleteWriter`和`IcebergFanoutPosDeleteWriter`。

# IcebergFanoutPosDeleteWriter

通过地址，得到一个List的posRows：
```java
List<PosRow<T>> posRows = posDeletes.get(wrapper.set(path));
if (posRows != null) {  
  posRows.add(PosRow.of(pos, row));  
} else {  
  posDeletes.put(CharSequenceWrapper.wrap(path),
		Lists.newArrayList(PosRow.of(pos, row)));  
}
```
如果posRows为null，说明这个文件在这一次优化中，还没有删除过数据。所以新建一个List，否则将本次优化导致删除的记录添加到posDeletes中。

实际的删除是在`flushDeletes`方法时触发的，其与关闭Writer的时机是一致的。在这个方法中，会使用`PositionDeleteWriter`完成处理。本质上，这些被删除的记录，会被



# ArcticTreeNodePosDeleteWriter

