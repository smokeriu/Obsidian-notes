梯度裁剪提供了一个快速修复**梯度爆炸**的方法。

对于长度为$T$的序列，我们计算梯度时，发生在遍历完后，此时会产生$\mathcal{O}(T)$的矩阵**乘法**链，正如[[../../多层感知机/不稳定的梯度|不稳定的梯度]]中提到的一样，过长的$T$可能导致数值不稳定， 例如可能导致梯度爆炸或梯度消失。

假定在向量形式的$\mathbf{x}$中， 或者在小批量数据的负梯度$\mathbf{g}$方向上。使用常数$\eta > 0$作为学习率，则在一次迭代中，$\mathbf{x} \gets \mathbf{x} - \eta \mathbf{g}$。我们假设目标函数表现良好——函数$f$在常数$L$上是*利普希茨连续*的。即：
$$
|f(\mathbf{x}) - f(\mathbf{y})| \leq L \|\mathbf{x} - \mathbf{y}\|
$$
> $\|\mathbf{x} - \mathbf{y}\|$是[[../../../../数学/基础/向量与图形/范数|范数]]的符号

特别的，有：
$$
|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta\|\mathbf{g}\|
$$

这意味着我们不会观察到超过$L \eta\|\mathbf{g}\|$的变化。 这既是坏事也是好事。 
- 坏的方面，它限制了取得进展的速度； 
- 好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。

梯度过大时，从而优化算法可能无法收敛。我们可以降低$\eta$来解决，不过这样做，对于，一个*通用*的办法是，将梯度$\mathbf{g}$投影回给定半径 （例如$\theta$）的球来裁剪梯度$\mathbf{g}$：
$$
\mathbf{g} \gets \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}
$$

这样：
1. 梯度的[[../../../../数学/基础/向量与图形/范数|范数]]永远不会超过$\theta$。
> 当$\mathbf{g}$的范数大于$\theta$时，则会被裁剪为$\theta$。反之，维持不变。

2. 更新后的梯度完全与$\mathbf{g}$的原始方向对齐。
> 更新没有修改梯度的方向


