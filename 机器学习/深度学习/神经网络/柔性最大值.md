柔性最大值的想法其实就是为神经网络定义一种新式的输出层。其也是构建在[[反向传播#带权输入]]定义之上，但不使用S型函数。
# 定义
根据柔性最大值函数的定义，第$j$个神经元的激活值$a_j^L$就是：
$$
a_j^L =
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}}
$$
其中，分母的求和是在所有的输出神经元上进行的。
# 特点
## 所有输出神经元的激活值相加为1
我们可以通过公式证明这一点，且正因如此，当我们增大某一个带权输入，会导致其他输入对应的激活值降低，而增加带权输入对应的激活值增加。
## 输出激活值都是正数
因为指数函数是正的。
## 输出可以被看做 是一个概率分布
柔性最大值层的输出是一些相加为 1 正数的集合。
## 柔性最大值具有单调性
当我们增大某一个带权输入，会导致其他输入对应的激活值降低，而增加带权输入对应的激活值增加。
这意味着，当$j=k$时，$\partial{a_j^L}/\partial{z_k^L} > 0$，反之则$<0$。
### 证明
1. $j=k$时。
$$
\partial{a_j^L}/\partial{z_k^L}
=
\frac{e^{z_j^L}(\sum_k e^{z_k^L})- e^{z_j^L}(\sum_k e^{z_k^L})^{'}}
{(\sum_k e^{z_k^L})^2}
=
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}} - 
\frac{e^{z_j^L}}{\sum_k e^{z_k^L}} 
\cdot
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
=
a_j^L - (a_j^L)^2
$$
由于$j=k$，则：
$$
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
=
\frac{e^{z_k^L}}{\sum_k e^{z_k^L}}
$$


2. $j \ne k$时。

## 柔性最大值的非局部性
根据定义，我们可以直到，任何特定的输出激活值$a_j^L$依赖于所有的带权输入。
> 而[[感知器和神经元#S型神经元]]只依赖于自身。