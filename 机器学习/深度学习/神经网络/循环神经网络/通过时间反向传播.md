深度学习利用[[../../多层感知机/反向传播|反向传播]]来完成对参数的更新，不过在循环神经网络中，直接使用反向传播，很容易遇到传播链过长的问题，因为$h_t$由$x_t$于$h_{t-1}$生成，这意味着序列末尾元素会反向传播到序列的第一个元素。
在[[循环神经网络]]中，我们知道计算$H_t$的公式为：
$$
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)
$$
其中，$\mathbf{W}_{hh}$是隐变量的权重参数，当我们对其求梯度时，则：
$$
\begin{split}\begin{aligned}\frac{\partial L}{\partial w_h}  & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h}  \\& = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}  \frac{\partial h_t}{\partial w_h}\end{aligned}\end{split}
$$
而对于$\frac{\partial h_t}{\partial w_h}$这部分，由于$H_t$和$H_{t-1}$的计算都依赖于$W_{hh}$，则：
$$
\frac{\partial h_t}{\partial w_h}= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}
$$
进一步推广到$H_1$，则有：
$$
\frac{\partial h_t}{\partial w_h}=\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}
$$
可以发现，后面这部分是一个连乘带连加的公式，这在实际计算中几乎是不可能实现的，而通过时间反向传播，实际上是循环神经网络中反向传播技术的一个特定应用。

# 截断时间步
比较常见的方案是，*固定*在$\tau$步后截断传播。 这样做导致该模型主要侧重于短期影响，而不是长期影响。 这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。

## 随机截断
进一步，我们可以用一个随机变量替换$\frac{\partial h_t}{\partial w_h}$，这个随机变量是通过使用序列$\xi_t$来实现的， 序列预定义了$0 \leq \pi_t \leq 1$， 其中$P(\xi_t = 0) = 1-\pi_t$且$P(\xi_t = \pi_t^{-1}) = \pi_t$， 因此$E[\xi_t] = 1$。则：
$$
z_t= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}
$$

用图表示这些方法的差异如下：
![[assets/Pasted image 20230912195040.png|500]]
实际上，随机截断的效果并不显得比固定截断要好，所以大部分情况下，会使用更简单的固定截断。

# 反向传播的细节
回顾下前向传播的公式：
$$
\begin{split}\begin{aligned}\mathbf{h}_t &= \mathbf{W}_{hx} \mathbf{x}_t + \mathbf{W}_{hh} \mathbf{h}_{t-1},\\
\mathbf{o}_t &= \mathbf{W}_{qh} \mathbf{h}_{t},\end{aligned}\end{split}
$$
用$l(\mathbf{o}_t, y_t)$表示时间步$t$时的损失函数，则得到总体损失为：
$$
L = \frac{1}{T} \sum_{t=1}^T l(\mathbf{o}_t, y_t).
$$
上述关系如图所示为：
![[assets/Pasted image 20230916161032.png|500]]
显然，反向传播需要计算梯度的是$\mathbf{W}_{hx}$、$\mathbf{W}_{hh}$和$\mathbf{W}_{qh}$。对于任意时间步$t$，是损失对输出进行微分，得到：
$$
\frac{\partial L}{\partial \mathbf{o}_t} =  \frac{\partial l (\mathbf{o}_t, y_t)}{T \cdot \partial \mathbf{o}_t} \in \mathbb{R}^q.
$$
> 需要注意，这里是对某一个时间步进行微分，而不是所有时间步，故结果没有$\sum$。

