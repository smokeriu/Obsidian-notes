# 代价方程
在实际中，我们需要将输入的数字图像转换为数字，这些数据来自于MNIST数据集，均为$28 \times 28$的灰度图像，所以，我们可将输入看做是$28 \times 28 = 784$维的向量。而输出则是十维的，$y(x) = (0,0,1,...,0)^{T}$，为一表示这个数字可能为对应的(坐标)代表的数字。
我们实际的目的，就是找到合适的权重$w$和偏置$b$，来使结果尽可能准确。为了量化目标，我们需要代价函数：
$$
\symbfit{C}(w,b) 
\equiv 
\frac{1}{2n} \sum_{x}{}\vert\vert y(x) -a \vert\vert ^2
$$
其中，$n$为训练的输入数据个数，$a$表示输入x时的输出向量。符号$\vert\vert \vec{v} \vert\vert$指向量$\vec{v}$的模。上述C被称为**二次**代价函数，也被称为**均方误差**或者**MSE**。其是一个非负数，所以其约趋近于0，则说明结果越准确。
> 使用二次的原因是，我们想放大对权重和偏置的微小改变对结果的影响。

# 梯度下降
使用[[../../../数学/基础/微积分/向量场、梯度、势能#梯度|向量场、梯度、势能]]下降的方法，找到极值点。因为我们希望找到最小的$\symbfit{C}$，所以我们希望其变化率$\Delta{\symbfit{C}}$为负。

进一步，我们先将$\symbfit{C}$简化，想象其为两个向量组成的函数。当我们在一个点，将其向$\vec{v_1}$和$\vec{v_2}$移动很小的量，可以表示为：
$$
\Delta{\symbfit{C}} 
\approx 
\frac{\partial{\symbfit{C}}}{\partial{v_1}} \Delta{v_1}
+
\frac{\partial{\symbfit{C}}}{\partial{v_2}} \Delta{v_2}
$$
特别的，我们定义一个$\Delta{v} \equiv (\Delta{v_1}, \Delta{v_2})^T$，为$\vec{v}$变化的向量。同时，我们对梯度进行定义：
$$
\nabla{\symbfit{C}} \equiv
(\frac{\partial{\symbfit{C}}}{\partial{v_1}},\frac{\partial{\symbfit{C}}}{\partial{v_2}})^T
$$
则，我们可以将$\Delta{\symbfit{C}}$改写为：
$$
\Delta{\symbfit{C}} 
\approx
\nabla{\symbfit{C}} · \Delta{v}
$$
我们的目的是选取何时的$\Delta{v}$，则选取：
$$
\Delta{v} = -\eta \nabla{\symbfit{C}}
$$
> $\eta$为学习速率，为一个很小的正数。可以通过[[../../../数学/基础/向量与图形/柯西—施瓦茨不等式|柯西—施瓦茨不等式]]证明上述等式是最佳的。
> 具体可参考[[手写数字识别#证明$ Delta{v}$的的取值公式|证明取值公式]]。

将上述两个方程结合，得到：
$$
\Delta{\symbfit{C}} 
\approx
\nabla{\symbfit{C}} · -\eta \nabla{\symbfit{C}}
=
-\eta · \vert\vert \nabla{\symbfit{C}} \vert\vert ^ 2
$$
所以，我们可以得到具体的梯度下降方程：
$$
v \to v^{'} = v + \Delta{v} = v -\eta \nabla{\symbfit{C}}
$$
实际上，这个方程可以扩展到更高的维度，而不是举例的二维。
所以，我们的目的之一是挑选一个合适的$\eta$，使一次移动合适的距离，过大的$\eta$可能导致一步过长，始终无法触底，甚至比初始点更高。而过小的$\eta$则会使梯度下降过慢。

## 计算梯度的问题
回到代价方程，其遍及每个样本的平均值得到的结果，而为了计算梯度，我们需要为每个输入单独的计算其值，然后得到平均的梯度值：$\nabla{\symbfit{C}} = \frac{1}{n}\sum_{x} \nabla{\symbfit{C_x}}$。这显然代价太大。
所以这里引入**随机梯度下降**的算法。其思想就是通过**随机选取小量训练**输入样本来计算梯度，从而估算梯度，而我们不断地选取训练集，直到所有输入都参与了估算梯度，此时我们称完成了一个训练**迭代期(epoch)**。然后我们就会开始一个新的训练迭代期。

随机选择的数据集大小，如果过大，会影响训练速度。如果过小，会导致两次训练间的梯度变化幅度大，导致整个训练收敛慢。

# 代码实现
$$
a^{’} = \sigma(wa + b)
$$
其中$a$为上一层的神经元激活向量，$w$为权重矩阵，$\sigma$为一个函数，称为向量化。可以证明其与[[感知器和神经元#S型神经元的定义]]是一个公式。



# 证明

## 证明$\Delta{v}$的的取值公式
梯度$\nabla{\symbfit{C}}$和$\Delta{v}$都是向量，有[[../../../数学/基础/向量与图形/柯西—施瓦茨不等式#定理一|柯西—施瓦茨不等式]]可知：
$$
(\nabla{\symbfit{C}} \cdot \Delta{v})^2
\leq
\vert\vert \nabla{\symbfit{C}} \vert\vert^2
\thinspace
\vert\vert \Delta{v} \vert\vert^2
$$
而等式成立的条件是$c_i = \lambda{v_i}, (i = 1,..,n)$，故需要$\nabla{\symbfit{C}} = \lambda \Delta{v}$成立，即$\frac{1}{\lambda} \cdot \nabla{\symbfit{C}} = \Delta{v}$成立。我们可以使用$\eta$替换，得到$\Delta{v}=\eta \cdot \nabla{\symbfit{C}}$。
我们的目的是：
- 找到$\symbfit{C}$减小幅度最大的方向，即$\Delta{\symbfit{C}}$变化最大。所以我们希望上述等式成立
- 我们希望$\Delta{\symbfit{C}}$为负，则$\eta$为负才会使得乘积为负，我们将负号提出来，即得到$\Delta{v} = -\eta \nabla{\symbfit{C}}$。

## 证明S型神经元公式
$$
\begin{eqnarray}
a^{’} &=& \sigma(wa + b) \\
&=& \sigma(\sum_j[w_1,...,w_n] \cdot [a_1,...,a_n] + b) \\

\end{eqnarray}
$$