# 代价方程
在实际中，我们需要将输入的数字图像转换为数字，这些数据来自于MNIST数据集，均为$28 \times 28$的灰度图像，所以，我们可将输入看做是$28 \times 28 = 784$维的向量。而输出则是十维的，$y(x) = (0,0,1,...,0)^{T}$，为一表示这个数字可能为对应的(坐标)代表的数字。
我们实际的目的，就是找到合适的权重$w$和偏置$b$，来使结果尽可能准确。为了量化目标，我们需要代价函数：
$$
\symbfit{C}(w,b) 
\equiv 
\frac{1}{2n} \sum_{x}{}\vert\vert y(x) -a \vert\vert ^2
$$
其中，$n$为训练的输入数据个数，$a$表示输入x时的输出向量。符号$\vert\vert \vec{v} \vert\vert$指向量$\vec{v}$的模。上述C被称为**二次**代价函数，也被称为**均方误差**或者**MSE**。其是一个非负数，所以其约趋近于0，则说明结果越准确。
> 使用二次方的原因是，我们想放大对权重和偏置的微小改变对结果的影响。

# 梯度下降
使用[[../../../数学/基础/微积分/向量场、梯度、势能#梯度|向量场、梯度、势能]]下降的方法，找到极值点。因为我们希望找到最小的$\symbfit{C}$，所以我们希望其变化率$\Delta{\symbfit{C}}$为负。

进一步，我们先将$\symbfit{C}$简化，想象其为两个向量组成的函数。当我们在一个点，将其向$\vec{v_1}$和$\vec{v_2}$移动很小的量，可以表示为：
$$
\Delta{\symbfit{C}} 
\approx 
\frac{\partial{\symbfit{C}}}{\partial{v_1}} \Delta{v_1}
+
\frac{\partial{\symbfit{C}}}{\partial{v_2}} \Delta{v_2}
$$
特别的，我们定义一个$\Delta{v} \equiv (\Delta{v_1}, \Delta{v_2})^T$，为$\vec{v}$变化的向量。同时，我们对梯度进行定义：
$$
\nabla{\symbfit{C}} \equiv
(\frac{\partial{\symbfit{C}}}{\partial{v_1}},\frac{\partial{\symbfit{C}}}{\partial{v_2}})^T
$$
则，我们可以将$\Delta{\symbfit{C}}$改写为：
$$
\Delta{\symbfit{C}} 
\approx
\nabla{\symbfit{C}} · \Delta{v}
$$
我们的目的是选取何时的$\Delta{v}$，则选取：
$$
\Delta{v} = -\eta \nabla{\symbfit{C}}
$$
> $\eta$为学习速率，为一个很小的正数。可以通过[[../../../数学/基础/向量与图形/柯西—施瓦茨不等式|柯西—施瓦茨不等式]]证明上述等式是最佳的。
> 具体可参考[[梯度下降#证明$ Delta{v}$的的取值公式|证明取值公式]]。

将上述两个方程结合，得到：
$$
\Delta{\symbfit{C}} 
\approx
\nabla{\symbfit{C}} · -\eta \nabla{\symbfit{C}}
=
-\eta · \vert\vert \nabla{\symbfit{C}} \vert\vert ^ 2
$$
所以，我们可以得到具体的梯度下降方程：
$$
v \to v^{'} = v + \Delta{v} = v -\eta \nabla{\symbfit{C}}
$$
实际上，这个方程可以扩展到更高的维度，而不是举例的二维。
所以，我们的目的之一是挑选一个合适的$\eta$，使一次移动合适的距离，过大的$\eta$可能导致一步过长，始终无法触底，甚至比初始点更高。而过小的$\eta$则会使梯度下降过慢。

## 总结
我们的目的是找到合适的$w$或$b$（为了方便，后续我们统称为$v$），使$\symbfit{C}(v)$尽可能的小，为此，我们假设我们初始选取的$v$距离最佳值有一段距离，此时**最佳的$\symbfit{C}$肯定比我们当前的$\symbfit{C}$更小**。所以我们要找到$\Delta{\symbfit{C}}$，且其为负数。为此我们对其在方向上进行微分，得到了
$$
\Delta{\symbfit{C}} 
\approx 
\frac{\partial{\symbfit{C}}}{\partial{v_1}} \Delta{v_1}
+
\frac{\partial{\symbfit{C}}}{\partial{v_2}} \Delta{v_2}
$$
我们的目的是是找到合适的$\Delta{v}$，使$\Delta{\symbfit{C}}$为负，经过上面的推理和证明，得到了：
$$
\Delta{v} = -\eta \nabla{\symbfit{C}}, \eta>0
$$
我们用现在的$v$，减去的算得到的变化的$v$，得到的就是更优化的值。而$\eta$只是一个常数，所以我们本质上是计算梯度。

# 计算梯度的问题
回到代价方程，其遍及每个样本的平均值得到的结果，而为了计算梯度，我们需要为每个输入单独的计算其值，然后得到平均的梯度值：$\nabla{\symbfit{C}} = \frac{1}{n}\sum_{x} \nabla{\symbfit{C_x}}$。这显然代价太大。
所以这里引入**随机梯度下降**的算法。其思想就是通过**随机选取小量训练**输入样本来计算梯度，从而估算梯度，而我们不断地选取训练集，直到所有输入都参与了估算梯度，此时我们称完成了一个训练**迭代期(epoch)**。然后我们就会开始一个新的训练迭代期。

## 随机梯度为何能得到正确值
随机梯度下降是一个取舍问题，他得到的不是正确值，而是一个从错误到正确的中间值。

随机选择一批数据，其能够代表我们数据的一些情况，并且一个epoch中，所有数据都参与了训练。无论我们单次训练选择的方向是否正确，但其主方向是向下的。而我们会在上一次的基础上进行额外的训练，他会使我们的结果逐渐趋近于正确的值。只是说如果epoch过小与随机值选择“恰好”不理想，我们离正确值会更远罢了

随机选择的数据集大小，如果过大，会影响训练速度。如果过小，会导致两次训练间的梯度变化幅度大，导致整个训练收敛慢。

# 代码实现
$$
a^{’} = \sigma(wa + b)
$$
其中$a$为上一层的神经元激活向量，$w$为权重矩阵，$\sigma$为一个函数，称为向量化。可以[[梯度下降#证明S型神经元公式|证明]]其与[[感知器和神经元#S型神经元的定义]]是一个公式。

所以，可以得到计算输出的代码，首先定义S型神经元：
```python
def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))
```
定义一个feedforward方法，接受输入，并返回输出：
```python
def feedforward(self, a):
	for b, w in zip(self.biases, self.weights):
	    a = sigmoid(np.dot(w, a)+b)
    
	return a
```
这里就是将上述公式翻译为Python代码。

最主要的是使用python实现随机梯度下降的代码：
```python
def SGD(self, training_data, epochs, mini_batch_size, eta,
		test_data=None):

	if test_data: n_test = len(test_data) # 得到测试数据长度
	n = len(training_data) # 训练数据长度，为一个二元组，表示输入和输出。
	for j in xrange(epochs): # 训练这么多次
		random.shuffle(training_data) # 打乱训练数据
		mini_batches = [ # 得到这一个周期的所有随机训练数据
			training_data[k:k+mini_batch_size]
			for k in xrange(0, n, mini_batch_size)] 
		for mini_batch in mini_batches:
			self.update_mini_batch(mini_batch, eta) # 分批次训练
		if test_data:
			print "Epoch {0}: {1} / {2}".format(
				j, self.evaluate(test_data), n_test)
		else:
			print "Epoch {0} complete".format(j)
```

其中，我们采用mini_batch的方式实现随机梯度下降：
```python
def update_mini_batch(self, mini_batch, eta):
	# 为每一层生成偏置和权重，其初始为0.
	nabla_b = [np.zeros(b.shape) for b in self.biases] 
	nabla_w = [np.zeros(w.shape) for w in self.weights]

	for x, y in mini_batch:
		# 累加训练得到的各个神经元对于不同输入的梯度
		delta_nabla_b, delta_nabla_w = self.backprop(x, y) # 计算梯度
		nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
		nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
	# 已有的权重，减本次得到的梯度均值*学习系数，得到修改后的权重结果
	self.weights = [w-(eta/len(mini_batch))*nw
					for w, nw in zip(self.weights, nabla_w)]
	self.biases = [b-(eta/len(mini_batch))*nb
					for b, nb in zip(self.biases, nabla_b)]
```
> backprop用于返回计算得到的梯度结果，采用的是[[反向传播]]算法实现的，但我们暂时忽略其实现。他的返回值是表示每一个神经元的输入权重(多个)和偏置所组成的向量。

> 这里使用已有权重(偏置)，减去本次得到的平均权重(偏置)，是基于上面得到的梯度下降方程。

# 证明

## 证明$\Delta{v}$的的取值公式
梯度$\nabla{\symbfit{C}}$和$\Delta{v}$都是向量，有[[../../../数学/基础/向量与图形/柯西—施瓦茨不等式#定理一|柯西—施瓦茨不等式]]可知：
$$
(\nabla{\symbfit{C}} \cdot \Delta{v})^2
\leq
\vert\vert \nabla{\symbfit{C}} \vert\vert^2
\thinspace
\vert\vert \Delta{v} \vert\vert^2
$$
而等式成立的条件是$c_i = \lambda{v_i}, (i = 1,..,n)$，故需要$\nabla{\symbfit{C}} = \lambda \Delta{v}$成立，即$\frac{1}{\lambda} \cdot \nabla{\symbfit{C}} = \Delta{v}$成立。我们可以使用$\eta$替换，得到$\Delta{v}=\eta \cdot \nabla{\symbfit{C}}$。
我们的目的是：
- 找到$\symbfit{C}$减小幅度最大的方向，即$\Delta{\symbfit{C}}$变化最大。所以我们希望上述等式成立
- 我们希望$\Delta{\symbfit{C}}$为负，则$\eta$为负才会使得乘积为负，我们将负号提出来，即得到$\Delta{v} = -\eta \nabla{\symbfit{C}}$。

## 证明S型神经元公式
$$
\begin{align}
a^{’} &= \sigma(wa + b) \\
&= \sigma(\sum_j[w_1,...,w_n] \cdot [a_1,...,a_n] + b) \\
&= \frac{1}{1+\exp(-\sum_j{w_jx_j} - b)}

\end{align}
$$