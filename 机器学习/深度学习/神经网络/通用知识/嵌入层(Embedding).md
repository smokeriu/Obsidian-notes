简单来说，嵌入层将正整数（下标）转换为具有固定大小的向量。

使用嵌入层的原因如下：
- 使用[[../../通用知识/独热编码|独热编码]]编码的向量会很高维也很稀疏。
- 训练过程中，每个嵌入的向量都会得到更新。

# 例子说明
1. 假设有如下句子：`Deep Learning is very deep`
2. 使用嵌入层embedding 的第一步是通过索引对该句子进行编码，这里我们给每一个不同的单词分配一个索引，上面的句子就会变成这样：`1, 2, 3, 4, 1`。
3. 接下来会创建嵌入矩阵，要决定每一个索引需要分配多少个*潜在因子*，这大体上意味着我们想要多长的向量，通常使用的情况是长度分配为32和50。这里，为了可读性，我们使用6个潜在因子。则我们需要为索引`1,2,3,4`分别分配6个索引，最终形成$4 \times 6$的矩阵。
![[assets/Pasted image 20231007162545.png]]

这样，我们就可以使用嵌入矩阵而不是庞大的独热编码向量来保持每个向量更小。简而言之，嵌入层embedding在这里做的就是把单词`deep`用向量`[.32, .02, .48, .21, .56, .15]`来表达。然而并不是每一个单词都会被一个向量来代替，而是被替换为用于查找嵌入矩阵中向量的索引。

参考：
- [Why You Need to Start Using Embedding Layers](https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12)