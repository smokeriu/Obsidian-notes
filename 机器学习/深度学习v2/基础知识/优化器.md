优化器，是指在反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。

优化器依赖于梯度下降：
$$
g_t = 
$$

优化器主要有两类：随机梯度下降（SGD）和自适应学习率（Ada）。
# 随机梯度下降
随机梯度下降到理念是，当训练出现震荡时，此时减小学习率，使得模型可以快速收敛。其公式为：
$$
\theta = \theta - v_t
$$
其中$v_t$为一个持有方向的变量，为：
$$
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)
$$
其中：

解释：
1. 当梯度$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$为正值，说明处于梯度下降的过程中，则会使得$v_t$越来越大，则更新速度越来越快。
2. 当梯度发生震荡，则此时，$v_{t-1}$仍然为上一次的值，但$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$会变为负值，这使得$v_t$不断变小，直到变回正确的方向。
3. 如此不断逼近局部最优点。

## pyTorch
pytorch的优化器位于torch.optim中。
```python
# model为待优化的模型
opt = torch.optim.SGD(model.parameters(), lr=HP.init_lr, momentum=HP)

优
```

# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。公式为：
$$
\begin{array}{l}
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i} \\
G_{t, i i}=G_{t-1, i i}+g_{t, i}^{2}
\end{array}
$$
其中：
- $g_{t,i}$为第$t$步的梯度。
- $\theta_{t,i}$表示第$t$步的第$i$个参数。

解释：
1. 由于$\mathbf{G_{t,ii}}$为上一步的$\mathbf{G}$加上当前步梯度的平方。这使得其会不断增大。导致$\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}}$越来越小，即学习率越来越小。
2. 这使得随着学习次数的增加，其能在后期避免模型震荡。

另外，由于Ada针对每个参数控制了学习率，这使得其适合稀疏的数据，这也可以更有效的学习需要的数据，（对于稀疏的数据，一些不必要的信息会很快收敛，而不单独控制学习率，会使得一些需要继续学习信息的梯度下降很难快速收敛）

## pyTorch

# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。