嵌入层的作用是，为一个简单的存储*固定大小的词典*的嵌入向量的查找表。
具体而言，给定一个编号
方法签名：
```python
torch.nn.Embedding(num_embeddings: int, 
				   embedding_dim: int,
				   padding_idx=None,
				   max_norm=None,  
				   norm_type=2.0,   
				   scale_grad_by_freq=False, 
				   sparse=False,  
				   _weight=None)
```
其中：
- `num_embeddings`：词典的大小尺寸，比如总共出现5000个词，那就输入5000。
- `embedding_dim`：嵌入向量的维度，即用多少维来表示一个符号。
- `padding_idx`：填充id。
	- 比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。
- `max_norm`：最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。
- `norm_type`：指定利用什么范数计算，并用于对比max_norm，默认为2范数。
- `scale_grad_by_freq`：根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.
- `sparse`：若为True,则与权重矩阵相关的梯度转变为稀疏张量。

