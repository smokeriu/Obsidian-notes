# 非参数注意力汇聚
汇聚注意力时，需要根据输入的位置对输出进行加权，有如下公式：
$$
f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i
$$
其中，$K$称为核回归。将公式以注意力机制的角度重写，得到：
$$
f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i
$$
其中：
- $x$是查询。
- $(x_i, y_i)$是一对键值对。
上述公式是$y_i$的加权平均，将查询$x$和键$x_i$的关系**建模**为注意力权重，即$\alpha(x, x_i)$，这个权重将被分配给每一个对应值$y_i$，并相加得到最终的结果。因此，对于任何查询，模型在所有*键值对*注意力权重，都是一个有效的概率分布： 它们是非负的，并且总和为1。

接着，考虑一个高斯核：
$$
K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).
$$
带入公式，有：
$$
\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}
$$
其中：
- 分母和分子的$\frac{1}{\sqrt{2\pi}}$被约去。
- 