> 又名Dropout。

在计算后续层之前向网络的*每一层注入噪声*。就是暂退法。

# 由来
## 重新审视过拟合
当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。不幸的是，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。

泛化性和灵活性之间的这种基本权衡被描述为*偏差-方差权衡*。线性模型有很高的偏差，但方差很低。
- 偏差很高：只能表示一小类函数。
- 方差很低：在不同的随机数据样本上可以得出相似的结果。

而我们的目标就是提高泛化性。
## 扰动的稳健性
经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以*简单的模型*为目标。
- 以较小维度的形式展现。
- 平滑性，即函数不应该对其输入的微小变化敏感。即*添加一些噪声*应当不会影响结果。
	- 已经从数学证明具有输入噪声的训练等价于Tikhonov正则化。

为此，在计算后续层之前向网络的每一层注入噪声，当训练一个有多层的深层网络时，注入噪声只会在`输入-输出`映射上*增强平滑性*。

因为我们从表面上看是在训练过程中丢弃一些神经元，所以被称为暂退法。标准暂退法包括在计算下一层之前*将当前层中的一些节点置零*。
# 定义
