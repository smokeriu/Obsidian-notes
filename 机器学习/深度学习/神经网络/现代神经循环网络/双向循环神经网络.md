在序列学习中，我们以往假设的目标是： 在给定观测的情况下 （例如，在时间序列的上下文中或在语言模型的上下文中）， 对下一个输出进行建模。 虽然这是一个典型情景，但不是唯一的。
例如上一个句子开头是Green，但我们需要通过*下文*才能够直到Green代表的是一个人还是绿色。


# 隐马尔可夫模型的动态规划
假设在任意时间步$t$，假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$来控制我们观察到的$x_t$。此外，任何$h_t \to h_{t+1}$转移都是有一些状态转移概率$P(h_{t+1} \mid h_t)$给出的，则称为**隐马尔可夫模型**，如图所示：
![](Pasted%20image%2020230924122059.png)
因此，对于有$T$个观测值的序列（一般指长度为$T$的序列），我们在观测状态和隐状态上具有以下联合概率分布可以通过公式给出：
$$
P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1)
$$

假设通过$x_{-j}$表示非时间步$j$的观测所组成的向量，即：
$$
x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})
$$
显然，目标其实是计算$P(x_j \mid x_{-j})$。由于没有观测到时间步$j$，$P(x_j \mid x_{-j})$不含隐变量，因此考虑对$h_1, \ldots, h_T$选择构成的*所有可能*的组合进行求和。而如果H