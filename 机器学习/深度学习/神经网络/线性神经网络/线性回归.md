**回归**，是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示**输入和输出之间的关系**。

# 基本元素
线性回归基于几个简单的假设：
- 假设自变量$x$和因变量$y$之间的关系是线性的， 即$y$可以表示为$x$中元素的加权和，这里通常允许包含观测值的一些噪声。
- 假设任何噪声都比较正常，如噪声遵循正态分布。
通常，我们使用$n$来表示数据集中的样本数。 对索引为$i$的样本，其输入表示为：
$$
x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^T
$$
其对应的标签为：$y^{(i)}$。
## 线性模型
对于线性模型，有如下定义：
$$
\hat{y} = w_1x_1+ ... + w_dx_d +b
$$
其中：
- $\hat{y}$表示$y$的估计值。
- $x_d$表示第$d$个特征。
- $w_d$表示对于$x_d$特征的权重。
- $b$表示偏置。
> 我们用b来表示特征数，n来表示样本数。
### 向量表示
特别的，我们将特征放到向量$\mathbf{x}$中，将权重放入向量$\mathbf{w}$中，则可以用**点积**简写为：
$$
\hat{y}= \mathbf{w}^T \cdot \mathbf{x} + b
$$
### 矩阵表示
特别的，将上述定义拓展到一个数据集，我们定义矩阵$\mathbf{X}$，表示有n个样本特征向量$\mathbf{x}$组成，矩阵中的每一行是一个样本，每一列是一种特征。则可以通过矩阵乘法表示上述定义：
$$
\hat{Y} = \mathbf{X} \mathbf{w} + b
$$
## 损失函数
_损失函数_（loss function）能够量化目标的*实际值*与*预测值*之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是*平方误差函数*：
### 平方误差函数
其定义为：
$$
l^{(i)} = \frac{1}{2} (\hat{y}^{(i)} - y^{(i)})^2
$$
其中：
- $y^{(i)}$表示预测值。
- $\hat{y}^{(i)}$表示实际的预估值。
- 这里取$\frac{1}{2}$只是为了简化求导后的复杂度。
对于数据集中的n个样本，其误差损失均值可以定义为：
$$
L(w,b) = 
\frac{1}{n} \sum_{i=1}^n l^{(i)}(w,b)
=
\frac{1}{n} \sum_{i=1}^n \frac{1}{2} (\mathbf{w}^T \mathbf{x}^{(i)} + b - y^{(i)})^2
$$
训练的目的，就是找到一组参数$(\mathbf{w}^*, b^*)$，使上述损失最小化。

## 解析解
线性回归的解可以用一个公式简单地表达出来， 这类解叫作**解析解**。
$$
\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.
$$
> 请注意，在计算时，y是一个

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

## 随机梯度下降
即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。即通过梯度下降等方式来找到近似解。

- 在每次迭代中，我们首先随机抽样一个小批量$\mathcal{B}$， 它是由固定数量的训练样本组成的。
- 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为**梯度**）。 
- 最后，我们将梯度乘以一个预先确定的正数$\eta$（学习率），并从当前参数的值中减掉。
用下面的数学公式来表示这一更新过程（$\partial$表示偏导数）：
$$
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).
$$
> 这里，我们求的是一组w和b，是的损失函数最小。根据梯度的定义，减去梯度，得到的新的w和b使损失函数更小。

特别的，
- 可以调整但不在训练过程中更新的参数称为**超参数**。如学习率和样本批量。
- 我们将调整超参数的过程称为**调参**。
- 

经过一定迭代周期的训练，我们可以得到估计的权重$\hat{w}$和偏置$\hat{b}$。
> 即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含**多个最小值**。事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为**泛化**。

