循环神经网络，是一类具备有**隐变量**的神经网络。

# 隐变量
由于序列的长度是不可控的，所以通过[[序列模型#马尔科夫假设]]，我们认为预测当前元素$x_t$，只与前$\tau$个元素有关。但对于之前的单词，也不希望其完全无用，所以和[[序列模型#潜变量模型]]中提及的类似，将之前的信息当做*隐变量*聚合后使用。

对于隐变量$h_t$，其存储了到时间步$t$为止的序列信息，一般而言，我们通过$h_{t-1}$与当前的输入$x_t$来共同更新$h_t$，即：
$$
h_t = f(x_{t}, h_{t-1})
$$


# 循环层
一般而言，我们可以简单认为循环层是隐藏层和隐变量的结合。

> 隐变量与隐藏层是不同的事物

在多层感知机中，我们的输出与输入、参数间的关系如下：
$$
\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h)
$$
其中，$\mathbf{H}$是*隐藏层*的输出，$\phi$是激活函数，其会作为输出层的输入：
$$
\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q
$$
如果是分类问题，我们可以用$\text{softmax}(\mathbf{O})$来计算输出类别的概率分布。

我们在隐藏层中引入隐变量：
$$
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)
$$
其中：
- $\mathbf{H}_{t-1}$为时间步$t-1$的隐变量，且$\mathbf{H}_t \in \mathbb{R}^{n \times h}$。
- $\mathbf{W}_{hh}$为新的权重参数，且$\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$。
可以看到，当前时间步的输入与前一个时间步的输出$\mathbf{H}_{t-1}$有关系，具体地说，当前时间步隐藏变量由*当前时间步的输入* 与*前一个时间步的隐藏变量*一起计算得出，可以将其看做是一种状态，故也称为**隐状态**。并且，这是循环的。所以称这一层为循环层，称这类网络是循环神经网络。

特别的，即使在不同的时间步，循环神经网络也总是使用这些模型参数。 因此，循环神经网络的参数开销不会随着时间步的增加而增加。

# 循环网络
![[assets/Pasted image 20230906153647.png]]
上图是一个典型的循环神经网络样式，展示了在三个相邻时间步的计算逻辑：
1. 拼接当前时间步$t$的输入$\mathbf{X}_t$和前一时间步$t-1$的隐状态$\mathbf{H}_{t-1}$。
2. 将拼接的结果送入带有激活函数$\phi$的全连接层，全连接层的输出是当前时间步$t$的隐状态$\mathbf{H}_t$。
3. 全连接层的输出$\mathbf{H}_t$将送至下一个时间步$t+1$中参与计算。并且其自身也会*直接*送至输出层，用于计算当前时间步$t$的输出$\mathbf$。