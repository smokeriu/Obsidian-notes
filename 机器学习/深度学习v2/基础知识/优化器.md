优化器主要有两类：随机梯度下降（SGD）和自适应学习率（Ada）。
# 随机梯度下降
随机梯度下降到理念是，当训练出现震荡时，此时减小学习率，使得模型可以快速收敛。其公式为：
$$
\theta = \theta - v_t
$$
其中$v_t$为一个持有方向的变量，为：
$$
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)
$$
其中：

解释：
1. 当梯度$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$为正值，说明处于梯度下降的过程中，则会使得$v_t$越来越大，则更新速度越来越快。
2. 当梯度发生震荡，则此时，$v_{t-1}$仍然为上一次的值，但$$

# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。


# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。