# 思想背景
随着函数的复杂，可能不会使网络更接近理想的结果：
![](Pasted%20image%2020230827173318.png)
对于左侧的情况，随着函数的复杂，其能够学习的范围扩大，但并没有离最佳结果（五角星）更近，所以，我们实际需要的泛函是第二类，即随着函数的复杂，其至少需要包含原来函数的表达范围，这样，更复杂的函数至少不会比原来函数结果更差。

ResNet正是这种思想的具体实现，其主要思想是：

我们假设函数输入为$x$，则有$f(x)$，我们在其上增加一级，得到：
$$
g(x) = x + f(x)
$$
这样，则$g(x)$一定包含了输入的$x$。即至少不会比原来差：

> 如果$f(x)$会导致结果变坏，则随着训练的进行，其得到的权重会逐渐减小（训练是朝着变好的方向进行），则最终，$g(x) \approx x$。

![](Pasted%20image%2020230827175533.png)

实际训练过程中，因为$g(x)$中，$x$比$f(x)$更简单，所以其更容易被训练，

# ResNet块
尽管ResNet的思想是**通用的**，不过其最初的版本是沿用了[使用块的网络(VGG)](使用块的网络(VGG).md)实现的，在具体的实现上，一般有两种选择——是否增加[1x1卷积层](1x1卷积层.md)：
![](Pasted%20image%2020230827182128.png)
