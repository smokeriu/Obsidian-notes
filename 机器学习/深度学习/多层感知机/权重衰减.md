在[[泛化和过拟合#模型复杂性]]中提到，模型过于复杂可能会导致过拟合，一个简单的处理途径是限制特征的数量（在多项式回归中体现为限制阶数）。但随着阶数$d$的增长，带有阶数$d$的项数会迅速增加。给定$k$个变量，阶数为$d$的项的个数为${k - 1 + d} \choose {k - 1}$，即$C^{k-1}_{k-1+d} = \frac{(k-1+d)!}{(d)!(k-1)!}$。
因此即使是阶数上的微小变化，比如从2到3，也会显著增加我们模型的复杂性。故而，简单的限制特征数量，会使得模型在过简单或过复杂之间徘徊。

# 范数和权重衰减
*权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。这项技术通过**函数与零的距离**来衡量函数的复杂度， 因为在所有函数$f$中，函数$f=0$（所有输入都得到值0） 在某种意义上是最简单的。

如何衡量函数与零的距离？一个简单的方法是，通过线性函数$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$中的权重向量的某个范数来度量其复杂性，例如$\| \mathbf{w} \|^2$。我们的目的就是使这个范数足够的小，而要保证权重向量比较小， 最常用方法是**将其范数作为惩罚项加到最小化损失**的问题中。这样，优化权重$w$的行为也会加入到学习过程中，

即将[[../神经网络/线性神经网络/损失函数|损失函数]]加上正则化，变更为：
$$
L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$
其中，$\lambda$本称为正则化*常数*，是一个*非负*超参数。较小的$\lambda$值对应较少约束的$\mathbf{w}$， 而较大的$\lambda$值对$\mathbf{w}$的约束更大。


> 这里取$\frac{1}{2}$只是为了求导后，与二次方约去。而这里选择$L_2$范数的原因是，它对权重向量的*大分量*施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。
> 
> 相比之下，$L_1$惩罚会导致模型将权重集中在*一小部分特征*上， 而将其他权重清除为零。 这称为特征选择（feature selection），这可能是其他场景下需要的。
> 特别的，$L_2$回归被称为岭回归，$L_1$回归被称为套索回归。

特别的，我们将公式展开，并使用[[../神经网络/线性神经网络/线性回归#平方误差函数|平方误差函数]]。则[[../神经网络/线性神经网络/线性回归#随机梯度下降|随机梯度下降]]可以写成：
$$
\begin{aligned}
\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right).
\end{aligned}
$$
通常，网络输出层的偏置项不会被正则化。
