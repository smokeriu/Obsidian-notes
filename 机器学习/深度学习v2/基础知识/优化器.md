优化器，是指在反向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。

优化器依赖于[梯度下降](梯度下降.md)：
1. 我们首先得到当前参数[梯度](梯度.md)$g_t$：
$$
g_t = \nabla f(w_t)
$$
2. 根据梯度计算一阶动量和二阶动量：
$$
\begin{array}{l}
m_t = \phi(g_1, g_2, \cdots, g_t) \\
\mathbf{V}_t = \sum_{i=0}^{t} x_t^2
\end{array}
$$
3. 计算当前步的下降梯度：
$$
\eta_t = \frac{\alpha \cdot m_t}{\sqrt{\mathbf{V}_t}}
$$
其中，$\alpha$一般为学习率。

4. 根据计算的下降梯度更新参数：
$$
w_{t+1} = w_t + \eta_{t}
$$

梯度下降衍生了3类算法：
1. 批量梯度下降（BGD）。
2. 随机梯度下降（SGD）。
3. 小批量梯度下降（Mini-batch GD）。
他们的区别主要是一次计算中，选择的样本个数。批量梯度下降每一次下降会计算随机梯度下降一次仅选择一个样本，**SGD由于每次参数更新仅仅需要计算一个样本的梯度**，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。
事实上，


在此之上，优化器的发展方向主要有两类：动量优化和自适应学习率。
# 动量优化
随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，
随机梯度下降到理念是，当训练出现震荡时，此时减小学习率，使得模型可以快速收敛。其公式为：
$$
\theta = \theta - v_t
$$
其中$v_t$为一个持有方向的变量，为：
$$
v_{t}=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)
$$
其中：

解释：
1. 当梯度$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$为正值，说明处于梯度下降的过程中，则会使得$v_t$越来越大，则更新速度越来越快。
2. 当梯度发生震荡，则此时，$v_{t-1}$仍然为上一次的值，但$\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right)$会变为负值，这使得$v_t$不断变小，直到变回正确的方向。
3. 如此不断逼近局部最优点。

## pyTorch
pytorch的优化器位于torch.optim中。
```python
# model为待优化的模型
opt = torch.optim.SGD(model.parameters(), lr=HP.init_lr, momentum=HP)

优
```

# 自适应学习率
自适应学习率的理念是，不断根据梯度，减小学习率，使得模型在初期能够很好的收敛，并避免在后期震荡。公式为：
$$
\begin{array}{l}
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i} \\
G_{t, i i}=G_{t-1, i i}+g_{t, i}^{2}
\end{array}
$$
其中：
- $g_{t,i}$为第$t$步的梯度。
- $\theta_{t,i}$表示第$t$步的第$i$个参数。

解释：
1. 由于$\mathbf{G_{t,ii}}$为上一步的$\mathbf{G}$加上当前步梯度的平方。这使得其会不断增大。导致$\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}}$越来越小，即学习率越来越小。
2. 这使得随着学习次数的增加，其能在后期避免模型震荡。

另外，由于Ada针对每个参数控制了学习率，这使得其适合稀疏的数据，这也可以更有效的学习需要的数据，（对于稀疏的数据，一些不必要的信息会很快收敛，而不单独控制学习率，会使得一些需要继续学习信息的梯度下降很难快速收敛）

## pyTorch

# 选择建议
面对稀疏的数据时，可以考虑使用自适应学习率。
面对较稠密的数据时，可以考虑使用随机梯度下降。