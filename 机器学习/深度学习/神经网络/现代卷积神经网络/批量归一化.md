一般而言，因为损失函数在最上面，数据在最下面，在反向传播时，靠近损失函数的层训练得更快，然而，底部的层不仅仅训练得慢，而且其一旦变动，顶部的层也会跟着变，这导致顶层被重复训练，最终导致收敛慢。
批量归一化主要用于解决这一问题，即学习底部层的时候，避免变化顶部层。

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：
- 在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。
- 应用比例系数和比例偏移。
正是由于这个基于*批量*统计的标准化，才有了*批量规范化*的名称。

> 请注意，如果我们尝试使用**大小为1**的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用**足够大的小批量**，批量规范化这种方法才是有效且稳定的。因此，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。从经验上，批量规范化最适应50∼100范围中的**中等批量大小**的难题。

# 公式

从形式上来说，用$\mathbf{x} \in \mathcal{B}$表示一个来自小批量$\mathcal{B}$的输入，批量规范化$BN$根据以下表达式转换$\mathbf{x}$：
$$
\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.
$$
> 这里使用的是[Hadamard乘积](Hadamard乘积.md)。

其中：
- $\hat{\boldsymbol{\mu}}_\mathcal{B}$是小批量$\mathcal{B}$的样本均值。
- $\hat{\boldsymbol{\sigma}}_\mathcal{B}$是小批量$\mathcal{B}$的样本标准差。
- $\boldsymbol{\gamma}$为拉伸参数，$\boldsymbol{\beta}$为偏移参数，是一组学习参数。
	- 这是因为单位方差（与其他一些魔法数）是一个主观的选择。
由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小

我们可以将部分参数展开：
$$
\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon.\end{aligned}\end{split}
$$
其中：
- $\epsilon$是一个非常小的常量，且$\epsilon > 0$。用于确保我们永远不会尝试除以零。

# 使用位置
## 全连接
作用于特征维上。

## 卷积层