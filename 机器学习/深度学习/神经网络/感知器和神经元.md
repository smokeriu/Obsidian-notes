# 感知器
所谓感知器，表示一个接受**n个**二进制输入，产生**一个**二进制输出的中间件。我们可以为不同输入设置不同的**权重**。
> 感知器的输入只能是0或1。

对于输入$x_1,x_2,x_3$，设置其权重分别为$w_1,w_2,w_3$，则我们可以得到一个总和$\sum_{j}{}w_jx_j$。在后续，将其简化写为$w · x$。对于一个简单地感知器，我们可以用其输出的结果与一个阈值进行比较，从而判断输出的结果。
$$
w · x \geq threshold, w · x < threshold 
$$
另外，我们可以将这个阈值移到等式坐标，得到一个称为偏置的值：
$$
w · x + b \geq 0, w · x + b < 0
$$

每一层可以包含多个感知器，而一个神经网络中包含了多层，其表现形式为：
![](Pasted%20image%2020230725232829.png)
> 需要说明的是，每个感知器只会产生一个输出，但可以将其分发给多个下游的感知器。上图中的多输出其实都是一个。
> 同样，同一个感知器的输出，而已两次作为下游感知器的输入，不过一般是将其权重进行合并。


# S型神经元
对权重（或者偏置）的微⼩的改动，都会引起感知器输出的微小改变。在输出不符合预期时，通过修改权重（或者偏置），来使输出符合预期，重复这个⼯作，反复改动权重和偏置来产⽣更好 的输出。这时⽹络就在学习。
不过问题在于当我们的⽹络包含感知器时这不会发⽣。实际上，⽹络中单个感知器上⼀个权重或偏置的微⼩改动有时候会引起那个感知器的输出完全翻转，如0变到1。
所以需要引⼊⼀种称为 S 型神经元的新的⼈⼯神经元来克服这个问题。

## S型神经元的定义
S型神经元有多个输入$x_1,x_2,...$，这些输入可以是`0到1`中的**任意值**，包括浮点数。其余输入和感知器一样，每个输入有权重，还有一个总的偏置。
而对于输出，其也不是0或1，而是$\sigma(w · x + b)$。这里的$\sigma$被称为S型函数，定义为：
$$
\sigma(z) \equiv \frac{1}{1+\mathrm{e}^{-z}}
$$
上述函数中，$z = \sum_j{w_jx_j} + b$。所以如果将输入、权重、偏置带入上述定义：
$$
\sigma(z) \equiv \frac{1}{1+\exp(-\sum_j{w_jx_j} - b)}
$$
在几何意义上，其函数形状如图：
![](Pasted%20image%2020230725235453.png)
> z越大，其值越接近于1；z越小，其值越接近于0。

# 神经网络架构
神经网络由多层神经元组成：
![[assets/Pasted image 20230726113732.png]]
上图中：
- 位于最左边的被称为输入神经元。对应的是输入层。
- 位于中间的被称为隐藏层，一个神经网络可以由多层隐藏层。
- 位于最右侧的被称为输出层，在上图中，输出层已有一个神经元。
> 由于历史原因，尽管上图中的神经元一般是S型神经元，但这种多层网络有时被称为**多层感知器**或者**MLP**。

更准确的讲，上图的神经网络都是以上一层的输出作为下一层的输入。这种网络被称为**前馈神经网络**。如果确 实有回路，我们最终会有这样的情况:$\sigma$函数的输入依赖于输出。这将难于理解，所以我们不允许这样的环路。
> 也有一些人工神经网络的模型，其中反馈环路是可行的。这些模型被称为**递归神经网络**。它的设计思想是，神经元会**休眠**。一个神经元在受到刺激后，只会被激活一段时间。这种设计下，一个神经元的输出只在*一段时间后*而不是即刻影响它的输入，所以回路并不会引起问题。

