> Softmax回归其实是一个分类问题。

Softmax一般会和[交叉熵代价函数](交叉熵代价函数.md)一起使用，即我们可以得到代价函数：
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = 
- \sum_i y_i log{\hat{y_i}}
$$
其中：
- $\mathbf{y}$是目标值，其是一个n维向量，且只有第$i$位为1，其余位为0。表示结果为第$i$位表示的分类项。
- $$