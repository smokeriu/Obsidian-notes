当我们再训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者值训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候我们就需要使用这个函数分离出来，避免反向传播。

detach会返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是`requires_grad`为false，得到的这个`tensor`永远不需要计算其梯度，不具有grad。

需要注意的是，即使我们为新的tensor请求梯度，**它也不会具有梯度grad**。