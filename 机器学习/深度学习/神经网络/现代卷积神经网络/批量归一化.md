一般而言，因为损失函数在最上面，数据在最下面，在反向传播时，靠近损失函数的层训练得更快，然而，底部的层不仅仅训练得慢，而且其一旦变动，顶部的层也会跟着变，这导致顶层被重复训练，最终导致收敛慢。
批量归一化主要用于解决这一问题，即学习底部层的时候，避免变化顶部层。

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：
- 在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。
- 应用比例系数和比例偏移。
正是由于这个基于*批量*统计的标准化，才有了*批量规范化*的名称。

> 请注意，如果我们尝试使用**大小为1**的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。所以，只有使用**足够大的小批量**，批量规范化这种方法才是有效且稳定的。因此，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。从经验上，批量规范化最适应50∼100范围中的**中等批量大小**的难题。

# 公式

从形式上来说，用$\mathbf{x} \in \mathcal{B}$表示一个来自小批量$\mathcal{B}$的输入，批量规范化$BN$根据以下表达式转换$\mathbf{x}$：
$$
\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.
$$
> 这里使用的是[Hadamard乘积](Hadamard乘积.md)。

其中：
- $\hat{\boldsymbol{\mu}}_\mathcal{B}$是小批量$\mathcal{B}$的样本均值。
- $\hat{\boldsymbol{\sigma}}_\mathcal{B}$是小批量$\mathcal{B}$的样本标准差。
- $\boldsymbol{\gamma}$为拉伸参数，$\boldsymbol{\beta}$为偏移参数，是一组学习参数。
	- 这是因为单位方差（与其他一些魔法数）是一个主观的选择。
由于在训练过程中，中间层的变化幅度不能过于剧烈，而批量规范化将每一层主动居中，并将它们重新调整为给定的平均值和大小

我们可以将部分参数展开：
$$
\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbf{x},\\
\hat{\boldsymbol{\sigma}}_\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon.\end{aligned}\end{split}
$$
其中：
- $\epsilon$是一个非常小的常量，且$\epsilon > 0$。用于确保我们永远不会尝试除以零。

# 实际原理
批量归一化，其实是通过在每个小批量中加入噪音，来控制模型的复杂度。


# 批量归一化层
## 全连接
通常，我们将批量规范化层置于全连接层中的输出和激活函数之间。即：
$$
\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) ).
$$
我们可以认为其直接作用于*特征维*上。

## 卷积
我们可以在卷积层之后和激活函数之前应用批量规范化。

当卷积有多个输出通道时，我们需要对这些通道的*每个*输出执行批量规范化，假设我们的小批量包含$m$个样本，并且对于每个通道，卷积的输出具有高度$p$和宽度$q$。 那么对于卷积层，我们在每个输出通道的$m \cdot p \cdot q$个元素上同时执行每个批量规范化。

即我们可以认为其作用在*通道维*上。


# 预测过程中的批量归一化
批量规范化在训练模式和预测模式下的行为*通常不同*：
- 将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。
- 我们可能需要使用我们的模型对逐个样本进行预测，而不是一批。
所以，一般通过[移动平均值](移动平均值.md)估算*整个训练数据集*的样本均值和方差，并在预测时使用它们得到确定的输出。

# 代码实现

## 方法定义
```python
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
```
其中：
- `X`即输入的一个批次的数据。
- `gamma`和`beta`为公式中的学习参数$\gamma$和$\beta$。
- `moving_mean`和`moving_var`为预测时使用的*整个训练数据集*的样本均值和方差。训练时不使用。
- `eps`为避免方差为0，导致除法异常的常数$\epsilon$。
- `momentum`用于更新`moving_mean`和`moving_var`的

## 获取均值和方差
对于全连接层，我们是直接作用于*特征维*上。则：
```python
mean = X.mean(dim=0)
var = ((X - mean) ** 2).mean(dim=0)
```
> 对于二维输入，是一个矩阵，其中每一行是一个样本，这里`dim=1`表示对每一列求均值，得到一个行向量。即为由每个特征均值组成的$1 \times n$的行向量。

对于卷积层，则作用于*通道维*上。则：
```python
mean = X.mean(dim=(0, 2, 3), keepdim=True)
var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
```
> 对于卷积，输入是一个4维张量，第一维表示一个样本，第二维是通道数，第三维和第四维则是平面矩阵，所以这里得到的是通道数的均值。最终的结果是一个$1 \times n \times 1 \times 1$的张量。